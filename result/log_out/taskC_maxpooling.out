nohup: ignoring input
PyTorch Version:  1.0.1.post2
Torchvision Version:  0.2.2
Net(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (maxpool2): MaxPool2d(kernel_size=7, stride=1, padding=0, dilation=1, ceil_mode=False)
  (fclass): Linear(in_features=2048, out_features=65, bias=True)
)
Initializing Datasets and Dataloaders...
Params to learn:
	 conv1.weight
	 bn1.weight
	 bn1.bias
	 layer1.0.conv1.weight
	 layer1.0.bn1.weight
	 layer1.0.bn1.bias
	 layer1.0.conv2.weight
	 layer1.0.bn2.weight
	 layer1.0.bn2.bias
	 layer1.0.conv3.weight
	 layer1.0.bn3.weight
	 layer1.0.bn3.bias
	 layer1.0.downsample.0.weight
	 layer1.0.downsample.1.weight
	 layer1.0.downsample.1.bias
	 layer1.1.conv1.weight
	 layer1.1.bn1.weight
	 layer1.1.bn1.bias
	 layer1.1.conv2.weight
	 layer1.1.bn2.weight
	 layer1.1.bn2.bias
	 layer1.1.conv3.weight
	 layer1.1.bn3.weight
	 layer1.1.bn3.bias
	 layer1.2.conv1.weight
	 layer1.2.bn1.weight
	 layer1.2.bn1.bias
	 layer1.2.conv2.weight
	 layer1.2.bn2.weight
	 layer1.2.bn2.bias
	 layer1.2.conv3.weight
	 layer1.2.bn3.weight
	 layer1.2.bn3.bias
	 layer2.0.conv1.weight
	 layer2.0.bn1.weight
	 layer2.0.bn1.bias
	 layer2.0.conv2.weight
	 layer2.0.bn2.weight
	 layer2.0.bn2.bias
	 layer2.0.conv3.weight
	 layer2.0.bn3.weight
	 layer2.0.bn3.bias
	 layer2.0.downsample.0.weight
	 layer2.0.downsample.1.weight
	 layer2.0.downsample.1.bias
	 layer2.1.conv1.weight
	 layer2.1.bn1.weight
	 layer2.1.bn1.bias
	 layer2.1.conv2.weight
	 layer2.1.bn2.weight
	 layer2.1.bn2.bias
	 layer2.1.conv3.weight
	 layer2.1.bn3.weight
	 layer2.1.bn3.bias
	 layer2.2.conv1.weight
	 layer2.2.bn1.weight
	 layer2.2.bn1.bias
	 layer2.2.conv2.weight
	 layer2.2.bn2.weight
	 layer2.2.bn2.bias
	 layer2.2.conv3.weight
	 layer2.2.bn3.weight
	 layer2.2.bn3.bias
	 layer3.0.conv1.weight
	 layer3.0.bn1.weight
	 layer3.0.bn1.bias
	 layer3.0.conv2.weight
	 layer3.0.bn2.weight
	 layer3.0.bn2.bias
	 layer3.0.conv3.weight
	 layer3.0.bn3.weight
	 layer3.0.bn3.bias
	 layer3.0.downsample.0.weight
	 layer3.0.downsample.1.weight
	 layer3.0.downsample.1.bias
	 layer3.1.conv1.weight
	 layer3.1.bn1.weight
	 layer3.1.bn1.bias
	 layer3.1.conv2.weight
	 layer3.1.bn2.weight
	 layer3.1.bn2.bias
	 layer3.1.conv3.weight
	 layer3.1.bn3.weight
	 layer3.1.bn3.bias
	 layer3.2.conv1.weight
	 layer3.2.bn1.weight
	 layer3.2.bn1.bias
	 layer3.2.conv2.weight
	 layer3.2.bn2.weight
	 layer3.2.bn2.bias
	 layer3.2.conv3.weight
	 layer3.2.bn3.weight
	 layer3.2.bn3.bias
	 layer4.0.conv1.weight
	 layer4.0.bn1.weight
	 layer4.0.bn1.bias
	 layer4.0.conv2.weight
	 layer4.0.bn2.weight
	 layer4.0.bn2.bias
	 layer4.0.conv3.weight
	 layer4.0.bn3.weight
	 layer4.0.bn3.bias
	 layer4.0.downsample.0.weight
	 layer4.0.downsample.1.weight
	 layer4.0.downsample.1.bias
	 layer4.1.conv1.weight
	 layer4.1.bn1.weight
	 layer4.1.bn1.bias
	 layer4.1.conv2.weight
	 layer4.1.bn2.weight
	 layer4.1.bn2.bias
	 layer4.1.conv3.weight
	 layer4.1.bn3.weight
	 layer4.1.bn3.bias
	 layer4.2.conv1.weight
	 layer4.2.bn1.weight
	 layer4.2.bn1.bias
	 layer4.2.conv2.weight
	 layer4.2.bn2.weight
	 layer4.2.bn2.bias
	 layer4.2.conv3.weight
	 layer4.2.bn3.weight
	 layer4.2.bn3.bias
	 fclass.weight
	 fclass.bias
Epoch 0/179
----------
train Loss: 10.4070 Acc: 0.0252
val Loss: 9.1105 Acc: 0.0114

Epoch 1/179
----------
train Loss: 5.4019 Acc: 0.0264
val Loss: 5.5094 Acc: 0.0205

Epoch 2/179
----------
train Loss: 5.0918 Acc: 0.0246
val Loss: 8.9292 Acc: 0.0114

Epoch 3/179
----------
train Loss: 4.8684 Acc: 0.0246
val Loss: 8.3791 Acc: 0.0091

Epoch 4/179
----------
train Loss: 4.7330 Acc: 0.0235
val Loss: 8.4820 Acc: 0.0183

Epoch 5/179
----------
train Loss: 4.6484 Acc: 0.0315
val Loss: 5.6612 Acc: 0.0251

Epoch 6/179
----------
train Loss: 4.5884 Acc: 0.0229
val Loss: 6.1070 Acc: 0.0274

Epoch 7/179
----------
train Loss: 4.5715 Acc: 0.0229
val Loss: 8.6574 Acc: 0.0228

Epoch 8/179
----------
train Loss: 4.5151 Acc: 0.0258
val Loss: 6.2556 Acc: 0.0342

Epoch 9/179
----------
train Loss: 4.4652 Acc: 0.0310
val Loss: 7.4787 Acc: 0.0228

Epoch 10/179
----------
train Loss: 4.4820 Acc: 0.0261
val Loss: 7.5421 Acc: 0.0228

Epoch 11/179
----------
train Loss: 4.4300 Acc: 0.0327
val Loss: 5.4296 Acc: 0.0228

Epoch 12/179
----------
train Loss: 4.4587 Acc: 0.0292
val Loss: 6.5282 Acc: 0.0205

Epoch 13/179
----------
train Loss: 4.3706 Acc: 0.0370
val Loss: 5.5257 Acc: 0.0297

Epoch 14/179
----------
train Loss: 4.3484 Acc: 0.0375
val Loss: 7.1601 Acc: 0.0548

Epoch 15/179
----------
train Loss: 4.3089 Acc: 0.0396
val Loss: 5.7915 Acc: 0.0411

Epoch 16/179
----------
train Loss: 4.2827 Acc: 0.0476
val Loss: 4.9563 Acc: 0.0594

Epoch 17/179
----------
train Loss: 4.2298 Acc: 0.0470
val Loss: 5.2601 Acc: 0.0479

Epoch 18/179
----------
train Loss: 4.2278 Acc: 0.0499
val Loss: 10.8706 Acc: 0.0479

Epoch 19/179
----------
train Loss: 4.2103 Acc: 0.0550
val Loss: 5.3939 Acc: 0.0708

Epoch 20/179
----------
train Loss: 4.1564 Acc: 0.0579
val Loss: 5.4292 Acc: 0.0548

Epoch 21/179
----------
train Loss: 4.1344 Acc: 0.0619
val Loss: 5.5123 Acc: 0.0662

Epoch 22/179
----------
train Loss: 4.1247 Acc: 0.0665
val Loss: 8.0052 Acc: 0.0616

Epoch 23/179
----------
train Loss: 4.1020 Acc: 0.0682
val Loss: 5.5771 Acc: 0.0594

Epoch 24/179
----------
train Loss: 4.0426 Acc: 0.0734
val Loss: 6.3726 Acc: 0.0822

Epoch 25/179
----------
train Loss: 4.0371 Acc: 0.0728
val Loss: 6.4312 Acc: 0.0799

Epoch 26/179
----------
train Loss: 4.0300 Acc: 0.0725
val Loss: 5.7600 Acc: 0.1096

Epoch 27/179
----------
train Loss: 3.9818 Acc: 0.0774
val Loss: 6.0488 Acc: 0.0845

Epoch 28/179
----------
train Loss: 3.9618 Acc: 0.0762
val Loss: 6.0486 Acc: 0.0799

Epoch 29/179
----------
train Loss: 3.9356 Acc: 0.0834
val Loss: 4.9798 Acc: 0.0845

Epoch 30/179
----------
train Loss: 3.9191 Acc: 0.0863
val Loss: 5.1108 Acc: 0.0890

Epoch 31/179
----------
train Loss: 3.8753 Acc: 0.0851
val Loss: 7.6134 Acc: 0.0959

Epoch 32/179
----------
train Loss: 3.9108 Acc: 0.0777
val Loss: 7.9659 Acc: 0.0639

Epoch 33/179
----------
train Loss: 3.8714 Acc: 0.0877
val Loss: 6.1825 Acc: 0.0799

Epoch 34/179
----------
train Loss: 3.8695 Acc: 0.0880
val Loss: 6.4334 Acc: 0.0890

Epoch 35/179
----------
train Loss: 3.8616 Acc: 0.0909
val Loss: 5.0076 Acc: 0.1073

Epoch 36/179
----------
train Loss: 3.8353 Acc: 0.0974
val Loss: 6.6218 Acc: 0.0959

Epoch 37/179
----------
train Loss: 3.7744 Acc: 0.1006
val Loss: 9.5942 Acc: 0.0959

Epoch 38/179
----------
train Loss: 3.7362 Acc: 0.1175
val Loss: 4.7612 Acc: 0.0936

Epoch 39/179
----------
train Loss: 3.7363 Acc: 0.1215
val Loss: 5.5686 Acc: 0.0913

Epoch 40/179
----------
train Loss: 3.7291 Acc: 0.1146
val Loss: 5.1626 Acc: 0.1301

Epoch 41/179
----------
train Loss: 3.7147 Acc: 0.1192
val Loss: 5.0833 Acc: 0.1461

Epoch 42/179
----------
train Loss: 3.6756 Acc: 0.1301
val Loss: 5.5727 Acc: 0.1347

Epoch 43/179
----------
train Loss: 3.6460 Acc: 0.1273
val Loss: 6.1269 Acc: 0.1598

Epoch 44/179
----------
train Loss: 3.6210 Acc: 0.1344
val Loss: 5.0498 Acc: 0.1507

Epoch 45/179
----------
train Loss: 3.5825 Acc: 0.1439
val Loss: 6.5942 Acc: 0.1370

Epoch 46/179
----------
train Loss: 3.6159 Acc: 0.1479
val Loss: 7.1879 Acc: 0.1210

Epoch 47/179
----------
train Loss: 3.5693 Acc: 0.1424
val Loss: 7.2550 Acc: 0.1301

Epoch 48/179
----------
train Loss: 3.5302 Acc: 0.1482
val Loss: 5.0871 Acc: 0.1461

Epoch 49/179
----------
train Loss: 3.5261 Acc: 0.1493
val Loss: 7.0431 Acc: 0.1553

Epoch 50/179
----------
train Loss: 3.5206 Acc: 0.1542
val Loss: 5.6296 Acc: 0.1347

Epoch 51/179
----------
train Loss: 3.5087 Acc: 0.1596
val Loss: 4.9518 Acc: 0.1712

Epoch 52/179
----------
train Loss: 3.4302 Acc: 0.1654
val Loss: 5.3599 Acc: 0.1461

Epoch 53/179
----------
train Loss: 3.4250 Acc: 0.1697
val Loss: 9.5416 Acc: 0.1689

Epoch 54/179
----------
train Loss: 3.4312 Acc: 0.1648
val Loss: 4.7982 Acc: 0.1986

Epoch 55/179
----------
train Loss: 3.4273 Acc: 0.1614
val Loss: 5.3894 Acc: 0.1553

Epoch 56/179
----------
train Loss: 3.3648 Acc: 0.1794
val Loss: 5.8813 Acc: 0.1918

Epoch 57/179
----------
train Loss: 3.3429 Acc: 0.1849
val Loss: 10.0358 Acc: 0.1826

Epoch 58/179
----------
train Loss: 3.2826 Acc: 0.1955
val Loss: 3.9112 Acc: 0.1941

Epoch 59/179
----------
train Loss: 3.2835 Acc: 0.1883
val Loss: 8.7262 Acc: 0.1758

Epoch 60/179
----------
train Loss: 3.3021 Acc: 0.1900
val Loss: 5.0945 Acc: 0.2100

Epoch 61/179
----------
train Loss: 3.2248 Acc: 0.2066
val Loss: 5.6481 Acc: 0.2055

Epoch 62/179
----------
train Loss: 3.2489 Acc: 0.2029
val Loss: 6.6637 Acc: 0.1986

Epoch 63/179
----------
train Loss: 3.1928 Acc: 0.2107
val Loss: 4.7858 Acc: 0.1781

Epoch 64/179
----------
train Loss: 3.1431 Acc: 0.2241
val Loss: 4.0452 Acc: 0.2260

Epoch 65/179
----------
train Loss: 3.1375 Acc: 0.2256
val Loss: 5.0416 Acc: 0.2352

Epoch 66/179
----------
train Loss: 3.1223 Acc: 0.2284
val Loss: 7.2184 Acc: 0.2283

Epoch 67/179
----------
train Loss: 3.1275 Acc: 0.2304
val Loss: 10.4941 Acc: 0.1781

Epoch 68/179
----------
train Loss: 3.0880 Acc: 0.2388
val Loss: 9.1469 Acc: 0.1781

Epoch 69/179
----------
train Loss: 3.0354 Acc: 0.2459
val Loss: 5.0770 Acc: 0.2603

Epoch 70/179
----------
train Loss: 3.0089 Acc: 0.2496
val Loss: 6.1747 Acc: 0.2329

Epoch 71/179
----------
train Loss: 3.0006 Acc: 0.2439
val Loss: 6.1877 Acc: 0.2215

Epoch 72/179
----------
train Loss: 2.9686 Acc: 0.2623
val Loss: 5.4444 Acc: 0.2763

Epoch 73/179
----------
train Loss: 2.9360 Acc: 0.2660
val Loss: 6.5141 Acc: 0.2237

Epoch 74/179
----------
train Loss: 2.9693 Acc: 0.2671
val Loss: 6.3754 Acc: 0.2489

Epoch 75/179
----------
train Loss: 2.9231 Acc: 0.2686
val Loss: 12.7240 Acc: 0.2420

Epoch 76/179
----------
train Loss: 2.9177 Acc: 0.2691
val Loss: 3.6969 Acc: 0.2991

Epoch 77/179
----------
train Loss: 2.8164 Acc: 0.2932
val Loss: 8.2175 Acc: 0.2603

Epoch 78/179
----------
train Loss: 2.8444 Acc: 0.2800
val Loss: 4.2279 Acc: 0.2831

Epoch 79/179
----------
train Loss: 2.8237 Acc: 0.2875
val Loss: 3.9480 Acc: 0.2808

Epoch 80/179
----------
train Loss: 2.7768 Acc: 0.3073
val Loss: 5.7540 Acc: 0.2854

Epoch 81/179
----------
train Loss: 2.7476 Acc: 0.3021
val Loss: 7.0525 Acc: 0.2991

Epoch 82/179
----------
train Loss: 2.7363 Acc: 0.3067
val Loss: 5.5781 Acc: 0.3059

Epoch 83/179
----------
train Loss: 2.7182 Acc: 0.3144
val Loss: 6.3461 Acc: 0.3105

Epoch 84/179
----------
train Loss: 2.6806 Acc: 0.3164
val Loss: 9.3326 Acc: 0.2557

Epoch 85/179
----------
train Loss: 2.7532 Acc: 0.3015
val Loss: 11.5036 Acc: 0.2763

Epoch 86/179
----------
train Loss: 2.6334 Acc: 0.3351
val Loss: 7.9155 Acc: 0.3037

Epoch 87/179
----------
train Loss: 2.6108 Acc: 0.3279
val Loss: 8.3490 Acc: 0.3219

Epoch 88/179
----------
train Loss: 2.6001 Acc: 0.3310
val Loss: 7.5568 Acc: 0.3333

Epoch 89/179
----------
train Loss: 2.5892 Acc: 0.3414
val Loss: 20.1873 Acc: 0.2831

Epoch 90/179
----------
train Loss: 2.6153 Acc: 0.3388
val Loss: 7.2119 Acc: 0.3242

Epoch 91/179
----------
train Loss: 2.5476 Acc: 0.3379
val Loss: 5.3493 Acc: 0.3562

Epoch 92/179
----------
train Loss: 2.5292 Acc: 0.3551
val Loss: 11.8694 Acc: 0.3333

Epoch 93/179
----------
train Loss: 2.4971 Acc: 0.3594
val Loss: 8.2515 Acc: 0.3219

Epoch 94/179
----------
train Loss: 2.4550 Acc: 0.3663
val Loss: 12.1882 Acc: 0.3311

Epoch 95/179
----------
train Loss: 2.4620 Acc: 0.3709
val Loss: 6.3235 Acc: 0.3470

Epoch 96/179
----------
train Loss: 2.3838 Acc: 0.3858
val Loss: 5.9079 Acc: 0.3242

Epoch 97/179
----------
train Loss: 2.3901 Acc: 0.3952
val Loss: 15.0561 Acc: 0.3379

Epoch 98/179
----------
train Loss: 2.3646 Acc: 0.3878
val Loss: 9.2231 Acc: 0.3584

Epoch 99/179
----------
train Loss: 2.3623 Acc: 0.4010
val Loss: 11.7036 Acc: 0.3082

Epoch 100/179
----------
train Loss: 2.3348 Acc: 0.4064
val Loss: 7.0092 Acc: 0.2763

Epoch 101/179
----------
train Loss: 2.2739 Acc: 0.4176
val Loss: 6.1110 Acc: 0.3676

Epoch 102/179
----------
train Loss: 2.2700 Acc: 0.4096
val Loss: 5.7756 Acc: 0.3082

Epoch 103/179
----------
train Loss: 2.2866 Acc: 0.4081
val Loss: 7.4546 Acc: 0.3721

Epoch 104/179
----------
train Loss: 2.2041 Acc: 0.4259
val Loss: 9.5150 Acc: 0.3470

Epoch 105/179
----------
train Loss: 2.1938 Acc: 0.4334
val Loss: 12.8928 Acc: 0.3470

Epoch 106/179
----------
train Loss: 2.1582 Acc: 0.4391
val Loss: 8.8755 Acc: 0.3904

Epoch 107/179
----------
train Loss: 2.1432 Acc: 0.4434
val Loss: 7.2097 Acc: 0.3995

Epoch 108/179
----------
train Loss: 2.1600 Acc: 0.4420
val Loss: 5.8159 Acc: 0.3858

Epoch 109/179
----------
train Loss: 2.1075 Acc: 0.4486
val Loss: 8.0172 Acc: 0.4087

Epoch 110/179
----------
train Loss: 2.0761 Acc: 0.4551
val Loss: 5.8718 Acc: 0.3881

Epoch 111/179
----------
train Loss: 2.0275 Acc: 0.4609
val Loss: 4.7966 Acc: 0.3995

Epoch 112/179
----------
train Loss: 2.0109 Acc: 0.4643
val Loss: 10.2464 Acc: 0.4178

Epoch 113/179
----------
train Loss: 2.0373 Acc: 0.4686
val Loss: 7.8860 Acc: 0.3836

Epoch 114/179
----------
train Loss: 2.0144 Acc: 0.4723
val Loss: 9.4671 Acc: 0.4087

Epoch 115/179
----------
train Loss: 1.9897 Acc: 0.4893
val Loss: 9.2309 Acc: 0.4087

Epoch 116/179
----------
train Loss: 1.9510 Acc: 0.4795
val Loss: 6.4614 Acc: 0.4110

Epoch 117/179
----------
train Loss: 1.9068 Acc: 0.4958
val Loss: 12.0205 Acc: 0.3584

Epoch 118/179
----------
train Loss: 1.9188 Acc: 0.4915
val Loss: 6.6144 Acc: 0.3219

Epoch 119/179
----------
train Loss: 1.8883 Acc: 0.5044
val Loss: 9.1524 Acc: 0.3447

Epoch 120/179
----------
train Loss: 1.8825 Acc: 0.4933
val Loss: 6.2427 Acc: 0.4064

Epoch 121/179
----------
train Loss: 1.8938 Acc: 0.4984
val Loss: 4.7424 Acc: 0.3721

Epoch 122/179
----------
train Loss: 1.8116 Acc: 0.5202
val Loss: 7.3610 Acc: 0.4064

Epoch 123/179
----------
train Loss: 1.8088 Acc: 0.5165
val Loss: 4.8965 Acc: 0.3950

Epoch 124/179
----------
train Loss: 1.8271 Acc: 0.5239
val Loss: 6.1459 Acc: 0.4132

Epoch 125/179
----------
train Loss: 1.7731 Acc: 0.5231
val Loss: 9.6775 Acc: 0.3950

Epoch 126/179
----------
train Loss: 1.7821 Acc: 0.5345
val Loss: 9.3050 Acc: 0.4018

Epoch 127/179
----------
train Loss: 1.7803 Acc: 0.5385
val Loss: 9.3604 Acc: 0.4178

Epoch 128/179
----------
train Loss: 1.7257 Acc: 0.5403
val Loss: 7.1584 Acc: 0.4680

Epoch 129/179
----------
train Loss: 1.7376 Acc: 0.5311
val Loss: 7.9391 Acc: 0.4041

Epoch 130/179
----------
train Loss: 1.6992 Acc: 0.5457
val Loss: 4.8967 Acc: 0.4338

Epoch 131/179
----------
train Loss: 1.6810 Acc: 0.5546
val Loss: 9.6907 Acc: 0.4475

Epoch 132/179
----------
train Loss: 1.7158 Acc: 0.5529
val Loss: 8.1370 Acc: 0.4475

Epoch 133/179
----------
train Loss: 1.6359 Acc: 0.5684
val Loss: 8.8876 Acc: 0.3470

Epoch 134/179
----------
train Loss: 1.5887 Acc: 0.5707
val Loss: 8.6513 Acc: 0.4315

Epoch 135/179
----------
train Loss: 1.5995 Acc: 0.5727
val Loss: 15.0931 Acc: 0.4635

Epoch 136/179
----------
train Loss: 1.5452 Acc: 0.5876
val Loss: 5.2681 Acc: 0.4658

Epoch 137/179
----------
train Loss: 1.5920 Acc: 0.5761
val Loss: 8.6069 Acc: 0.4521

Epoch 138/179
----------
train Loss: 1.5885 Acc: 0.5815
val Loss: 10.0994 Acc: 0.4269

Epoch 139/179
----------
train Loss: 1.5323 Acc: 0.5913
val Loss: 8.8038 Acc: 0.4269

Epoch 140/179
----------
train Loss: 1.5356 Acc: 0.5930
val Loss: 14.5440 Acc: 0.4247

Epoch 141/179
----------
train Loss: 1.5064 Acc: 0.5944
val Loss: 11.3149 Acc: 0.4498

Epoch 142/179
----------
train Loss: 1.4897 Acc: 0.6045
val Loss: 7.5144 Acc: 0.4475

Epoch 143/179
----------
train Loss: 1.4343 Acc: 0.6007
val Loss: 6.4177 Acc: 0.3836

Epoch 144/179
----------
train Loss: 1.4538 Acc: 0.6093
val Loss: 12.2100 Acc: 0.4247

Epoch 145/179
----------
train Loss: 1.4399 Acc: 0.6125
val Loss: 7.2922 Acc: 0.4406

Epoch 146/179
----------
train Loss: 1.4019 Acc: 0.6108
val Loss: 8.1311 Acc: 0.4863

Epoch 147/179
----------
train Loss: 1.4569 Acc: 0.6065
val Loss: 16.7561 Acc: 0.4247

Epoch 148/179
----------
train Loss: 1.4221 Acc: 0.6177
val Loss: 8.3978 Acc: 0.3881

Epoch 149/179
----------
train Loss: 1.4708 Acc: 0.5973
val Loss: 8.2192 Acc: 0.4566

Epoch 150/179
----------
train Loss: 1.3927 Acc: 0.6142
val Loss: 8.4232 Acc: 0.4658

Epoch 151/179
----------
train Loss: 1.4112 Acc: 0.6280
val Loss: 7.7406 Acc: 0.3927

Epoch 152/179
----------
train Loss: 1.4794 Acc: 0.6053
val Loss: 8.6595 Acc: 0.4521

Epoch 153/179
----------
train Loss: 1.3634 Acc: 0.6260
val Loss: 6.6995 Acc: 0.4726

Epoch 154/179
----------
train Loss: 1.3394 Acc: 0.6426
val Loss: 6.9771 Acc: 0.4292

Epoch 155/179
----------
train Loss: 1.4081 Acc: 0.6248
val Loss: 23.5539 Acc: 0.2169

Epoch 156/179
----------
train Loss: 1.6837 Acc: 0.5500
val Loss: 5.2663 Acc: 0.4703

Epoch 157/179
----------
train Loss: 1.4182 Acc: 0.6197
val Loss: 4.6786 Acc: 0.4863

Epoch 158/179
----------
train Loss: 1.3552 Acc: 0.6343
val Loss: 8.1403 Acc: 0.4954

Epoch 159/179
----------
train Loss: 1.3405 Acc: 0.6323
val Loss: 6.8772 Acc: 0.4498

Epoch 160/179
----------
train Loss: 1.3763 Acc: 0.6156
val Loss: 9.7490 Acc: 0.4795

Epoch 161/179
----------
train Loss: 1.3461 Acc: 0.6288
val Loss: 15.2312 Acc: 0.4909

Epoch 162/179
----------
train Loss: 1.2853 Acc: 0.6512
val Loss: 11.2790 Acc: 0.4635

Epoch 163/179
----------
train Loss: 1.3013 Acc: 0.6452
val Loss: 6.5095 Acc: 0.5046

Epoch 164/179
----------
train Loss: 1.2749 Acc: 0.6569
val Loss: 11.7318 Acc: 0.4703

Epoch 165/179
----------
train Loss: 1.2585 Acc: 0.6661
val Loss: 6.0885 Acc: 0.4087

Epoch 166/179
----------
train Loss: 1.2759 Acc: 0.6518
val Loss: 7.7397 Acc: 0.4452

Epoch 167/179
----------
train Loss: 1.2013 Acc: 0.6704
val Loss: 25.4497 Acc: 0.3653

Epoch 168/179
----------
train Loss: 1.2210 Acc: 0.6670
val Loss: 13.2378 Acc: 0.4954

Epoch 169/179
----------
train Loss: 1.2017 Acc: 0.6793
val Loss: 5.6124 Acc: 0.4498

Epoch 170/179
----------
train Loss: 1.2020 Acc: 0.6690
val Loss: 17.8120 Acc: 0.4110

Epoch 171/179
----------
train Loss: 1.1891 Acc: 0.6758
val Loss: 7.8922 Acc: 0.5137

Epoch 172/179
----------
train Loss: 1.1227 Acc: 0.6896
val Loss: 13.1999 Acc: 0.4566

Epoch 173/179
----------
train Loss: 1.1647 Acc: 0.6830
val Loss: 9.5886 Acc: 0.4886

Epoch 174/179
----------
train Loss: 1.1378 Acc: 0.6884
val Loss: 5.8867 Acc: 0.4977

Epoch 175/179
----------
train Loss: 1.2398 Acc: 0.6730
val Loss: 8.1845 Acc: 0.4269

Epoch 176/179
----------
train Loss: 1.1668 Acc: 0.6793
val Loss: 12.9860 Acc: 0.5046

Epoch 177/179
----------
train Loss: 1.1382 Acc: 0.6905
val Loss: 4.2759 Acc: 0.5479

Epoch 178/179
----------
train Loss: 1.1104 Acc: 0.6976
val Loss: 14.3334 Acc: 0.4817

Epoch 179/179
----------
train Loss: 1.0804 Acc: 0.7025
val Loss: 11.5368 Acc: 0.4635

Training complete in 115m 45s
Best val Acc: 0.547945
creating directory:  /workspace/ruilei/hw/result/taskC_maxpooling_2019-04-15-16_03
