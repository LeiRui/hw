nohup: ignoring input
PyTorch Version:  1.0.1.post2
Torchvision Version:  0.2.2
Net(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)
  (fclass): Linear(in_features=2048, out_features=65, bias=True)
)
Initializing Datasets and Dataloaders...
Params to learn:
	 conv1.weight
	 bn1.weight
	 bn1.bias
	 layer1.0.conv1.weight
	 layer1.0.bn1.weight
	 layer1.0.bn1.bias
	 layer1.0.conv2.weight
	 layer1.0.bn2.weight
	 layer1.0.bn2.bias
	 layer1.0.conv3.weight
	 layer1.0.bn3.weight
	 layer1.0.bn3.bias
	 layer1.0.downsample.0.weight
	 layer1.0.downsample.1.weight
	 layer1.0.downsample.1.bias
	 layer1.1.conv1.weight
	 layer1.1.bn1.weight
	 layer1.1.bn1.bias
	 layer1.1.conv2.weight
	 layer1.1.bn2.weight
	 layer1.1.bn2.bias
	 layer1.1.conv3.weight
	 layer1.1.bn3.weight
	 layer1.1.bn3.bias
	 layer1.2.conv1.weight
	 layer1.2.bn1.weight
	 layer1.2.bn1.bias
	 layer1.2.conv2.weight
	 layer1.2.bn2.weight
	 layer1.2.bn2.bias
	 layer1.2.conv3.weight
	 layer1.2.bn3.weight
	 layer1.2.bn3.bias
	 layer2.0.conv1.weight
	 layer2.0.bn1.weight
	 layer2.0.bn1.bias
	 layer2.0.conv2.weight
	 layer2.0.bn2.weight
	 layer2.0.bn2.bias
	 layer2.0.conv3.weight
	 layer2.0.bn3.weight
	 layer2.0.bn3.bias
	 layer2.0.downsample.0.weight
	 layer2.0.downsample.1.weight
	 layer2.0.downsample.1.bias
	 layer2.1.conv1.weight
	 layer2.1.bn1.weight
	 layer2.1.bn1.bias
	 layer2.1.conv2.weight
	 layer2.1.bn2.weight
	 layer2.1.bn2.bias
	 layer2.1.conv3.weight
	 layer2.1.bn3.weight
	 layer2.1.bn3.bias
	 layer2.2.conv1.weight
	 layer2.2.bn1.weight
	 layer2.2.bn1.bias
	 layer2.2.conv2.weight
	 layer2.2.bn2.weight
	 layer2.2.bn2.bias
	 layer2.2.conv3.weight
	 layer2.2.bn3.weight
	 layer2.2.bn3.bias
	 layer3.0.conv1.weight
	 layer3.0.bn1.weight
	 layer3.0.bn1.bias
	 layer3.0.conv2.weight
	 layer3.0.bn2.weight
	 layer3.0.bn2.bias
	 layer3.0.conv3.weight
	 layer3.0.bn3.weight
	 layer3.0.bn3.bias
	 layer3.0.downsample.0.weight
	 layer3.0.downsample.1.weight
	 layer3.0.downsample.1.bias
	 layer3.1.conv1.weight
	 layer3.1.bn1.weight
	 layer3.1.bn1.bias
	 layer3.1.conv2.weight
	 layer3.1.bn2.weight
	 layer3.1.bn2.bias
	 layer3.1.conv3.weight
	 layer3.1.bn3.weight
	 layer3.1.bn3.bias
	 layer3.2.conv1.weight
	 layer3.2.bn1.weight
	 layer3.2.bn1.bias
	 layer3.2.conv2.weight
	 layer3.2.bn2.weight
	 layer3.2.bn2.bias
	 layer3.2.conv3.weight
	 layer3.2.bn3.weight
	 layer3.2.bn3.bias
	 layer4.0.conv1.weight
	 layer4.0.bn1.weight
	 layer4.0.bn1.bias
	 layer4.0.conv2.weight
	 layer4.0.bn2.weight
	 layer4.0.bn2.bias
	 layer4.0.conv3.weight
	 layer4.0.bn3.weight
	 layer4.0.bn3.bias
	 layer4.0.downsample.0.weight
	 layer4.0.downsample.1.weight
	 layer4.0.downsample.1.bias
	 layer4.1.conv1.weight
	 layer4.1.bn1.weight
	 layer4.1.bn1.bias
	 layer4.1.conv2.weight
	 layer4.1.bn2.weight
	 layer4.1.bn2.bias
	 layer4.1.conv3.weight
	 layer4.1.bn3.weight
	 layer4.1.bn3.bias
	 layer4.2.conv1.weight
	 layer4.2.bn1.weight
	 layer4.2.bn1.bias
	 layer4.2.conv2.weight
	 layer4.2.bn2.weight
	 layer4.2.bn2.bias
	 layer4.2.conv3.weight
	 layer4.2.bn3.weight
	 layer4.2.bn3.bias
	 fclass.weight
	 fclass.bias
Epoch 0/179
----------
train Loss: 4.4628 Acc: 0.0330
val Loss: 4.2465 Acc: 0.0365

Epoch 1/179
----------
train Loss: 4.0803 Acc: 0.0413
val Loss: 10.4282 Acc: 0.0320

Epoch 2/179
----------
train Loss: 4.0203 Acc: 0.0453
val Loss: 7.1033 Acc: 0.0434

Epoch 3/179
----------
train Loss: 4.0132 Acc: 0.0467
val Loss: 5.7524 Acc: 0.0183

Epoch 4/179
----------
train Loss: 3.9818 Acc: 0.0482
val Loss: 4.0641 Acc: 0.0548

Epoch 5/179
----------
train Loss: 3.9414 Acc: 0.0550
val Loss: 3.9053 Acc: 0.0685

Epoch 6/179
----------
train Loss: 3.8807 Acc: 0.0596
val Loss: 4.1266 Acc: 0.0890

Epoch 7/179
----------
train Loss: 3.8665 Acc: 0.0585
val Loss: 4.0203 Acc: 0.0571

Epoch 8/179
----------
train Loss: 3.8512 Acc: 0.0608
val Loss: 4.0607 Acc: 0.0708

Epoch 9/179
----------
train Loss: 3.8561 Acc: 0.0679
val Loss: 3.8069 Acc: 0.0753

Epoch 10/179
----------
train Loss: 3.7903 Acc: 0.0699
val Loss: 3.8317 Acc: 0.0708

Epoch 11/179
----------
train Loss: 3.7662 Acc: 0.0800
val Loss: 3.7697 Acc: 0.0753

Epoch 12/179
----------
train Loss: 3.7769 Acc: 0.0814
val Loss: 3.7420 Acc: 0.1073

Epoch 13/179
----------
train Loss: 3.7081 Acc: 0.0886
val Loss: 3.7272 Acc: 0.1096

Epoch 14/179
----------
train Loss: 3.6681 Acc: 0.0957
val Loss: 3.8998 Acc: 0.0868

Epoch 15/179
----------
train Loss: 3.6015 Acc: 0.1069
val Loss: 3.7039 Acc: 0.1187

Epoch 16/179
----------
train Loss: 3.5932 Acc: 0.1118
val Loss: 3.5344 Acc: 0.1050

Epoch 17/179
----------
train Loss: 3.5451 Acc: 0.1192
val Loss: 3.8523 Acc: 0.1073

Epoch 18/179
----------
train Loss: 3.4645 Acc: 0.1316
val Loss: 3.5048 Acc: 0.1256

Epoch 19/179
----------
train Loss: 3.4534 Acc: 0.1353
val Loss: 3.9495 Acc: 0.1164

Epoch 20/179
----------
train Loss: 3.4442 Acc: 0.1381
val Loss: 3.3643 Acc: 0.1598

Epoch 21/179
----------
train Loss: 3.4239 Acc: 0.1502
val Loss: 3.5063 Acc: 0.1301

Epoch 22/179
----------
train Loss: 3.3928 Acc: 0.1522
val Loss: 3.3309 Acc: 0.2009

Epoch 23/179
----------
train Loss: 3.3458 Acc: 0.1614
val Loss: 3.2864 Acc: 0.1872

Epoch 24/179
----------
train Loss: 3.3133 Acc: 0.1751
val Loss: 3.3082 Acc: 0.1804

Epoch 25/179
----------
train Loss: 3.2665 Acc: 0.1780
val Loss: 3.4481 Acc: 0.1621

Epoch 26/179
----------
train Loss: 3.2664 Acc: 0.1757
val Loss: 3.3288 Acc: 0.1667

Epoch 27/179
----------
train Loss: 3.1883 Acc: 0.1909
val Loss: 3.7618 Acc: 0.1575

Epoch 28/179
----------
train Loss: 3.2075 Acc: 0.1915
val Loss: 3.2734 Acc: 0.1895

Epoch 29/179
----------
train Loss: 3.1347 Acc: 0.2081
val Loss: 3.0786 Acc: 0.2329

Epoch 30/179
----------
train Loss: 3.0713 Acc: 0.2112
val Loss: 3.4819 Acc: 0.1621

Epoch 31/179
----------
train Loss: 3.0566 Acc: 0.2207
val Loss: 3.0926 Acc: 0.2169

Epoch 32/179
----------
train Loss: 3.0170 Acc: 0.2310
val Loss: 3.0616 Acc: 0.2329

Epoch 33/179
----------
train Loss: 2.9824 Acc: 0.2322
val Loss: 3.0094 Acc: 0.2466

Epoch 34/179
----------
train Loss: 2.9572 Acc: 0.2410
val Loss: 3.1904 Acc: 0.2237

Epoch 35/179
----------
train Loss: 2.9852 Acc: 0.2244
val Loss: 3.1617 Acc: 0.2648

Epoch 36/179
----------
train Loss: 2.8796 Acc: 0.2571
val Loss: 3.2540 Acc: 0.2123

Epoch 37/179
----------
train Loss: 2.8684 Acc: 0.2534
val Loss: 3.0346 Acc: 0.2192

Epoch 38/179
----------
train Loss: 2.8217 Acc: 0.2737
val Loss: 3.0181 Acc: 0.2831

Epoch 39/179
----------
train Loss: 2.7585 Acc: 0.2777
val Loss: 3.2033 Acc: 0.2420

Epoch 40/179
----------
train Loss: 2.7320 Acc: 0.2901
val Loss: 2.8551 Acc: 0.2991

Epoch 41/179
----------
train Loss: 2.6735 Acc: 0.3044
val Loss: 2.9613 Acc: 0.3082

Epoch 42/179
----------
train Loss: 2.6186 Acc: 0.3118
val Loss: 2.8856 Acc: 0.2945

Epoch 43/179
----------
train Loss: 2.5938 Acc: 0.3256
val Loss: 3.2399 Acc: 0.2717

Epoch 44/179
----------
train Loss: 2.5854 Acc: 0.3262
val Loss: 2.8617 Acc: 0.2945

Epoch 45/179
----------
train Loss: 2.5493 Acc: 0.3299
val Loss: 2.8517 Acc: 0.3037

Epoch 46/179
----------
train Loss: 2.5435 Acc: 0.3299
val Loss: 2.7695 Acc: 0.3174

Epoch 47/179
----------
train Loss: 2.4846 Acc: 0.3522
val Loss: 2.7017 Acc: 0.3265

Epoch 48/179
----------
train Loss: 2.3918 Acc: 0.3844
val Loss: 2.8384 Acc: 0.3333

Epoch 49/179
----------
train Loss: 2.4079 Acc: 0.3732
val Loss: 2.8771 Acc: 0.3333

Epoch 50/179
----------
train Loss: 2.3514 Acc: 0.3858
val Loss: 2.5780 Acc: 0.3858

Epoch 51/179
----------
train Loss: 2.3088 Acc: 0.4001
val Loss: 3.2385 Acc: 0.2991

Epoch 52/179
----------
train Loss: 2.2711 Acc: 0.3987
val Loss: 3.0123 Acc: 0.3219

Epoch 53/179
----------
train Loss: 2.2639 Acc: 0.3975
val Loss: 2.5419 Acc: 0.3973

Epoch 54/179
----------
train Loss: 2.1930 Acc: 0.4107
val Loss: 2.6094 Acc: 0.3790

Epoch 55/179
----------
train Loss: 2.1744 Acc: 0.4133
val Loss: 2.6883 Acc: 0.3744

Epoch 56/179
----------
train Loss: 2.1266 Acc: 0.4302
val Loss: 5.1080 Acc: 0.2215

Epoch 57/179
----------
train Loss: 2.1137 Acc: 0.4228
val Loss: 2.6372 Acc: 0.3858

Epoch 58/179
----------
train Loss: 2.1004 Acc: 0.4371
val Loss: 2.7381 Acc: 0.3790

Epoch 59/179
----------
train Loss: 2.0603 Acc: 0.4557
val Loss: 2.4384 Acc: 0.4498

Epoch 60/179
----------
train Loss: 2.0007 Acc: 0.4580
val Loss: 2.4252 Acc: 0.4269

Epoch 61/179
----------
train Loss: 2.0015 Acc: 0.4675
val Loss: 2.8786 Acc: 0.3470

Epoch 62/179
----------
train Loss: 1.9644 Acc: 0.4635
val Loss: 2.7660 Acc: 0.3858

Epoch 63/179
----------
train Loss: 1.9234 Acc: 0.4864
val Loss: 2.4521 Acc: 0.4384

Epoch 64/179
----------
train Loss: 1.8722 Acc: 0.4947
val Loss: 3.3636 Acc: 0.3242

Epoch 65/179
----------
train Loss: 1.8375 Acc: 0.5062
val Loss: 2.6540 Acc: 0.4087

Epoch 66/179
----------
train Loss: 1.8169 Acc: 0.5148
val Loss: 2.9334 Acc: 0.3584

Epoch 67/179
----------
train Loss: 1.7693 Acc: 0.5248
val Loss: 2.3953 Acc: 0.4429

Epoch 68/179
----------
train Loss: 1.7558 Acc: 0.5257
val Loss: 2.2659 Acc: 0.4726

Epoch 69/179
----------
train Loss: 1.7341 Acc: 0.5360
val Loss: 2.8178 Acc: 0.4041

Epoch 70/179
----------
train Loss: 1.7352 Acc: 0.5322
val Loss: 2.8308 Acc: 0.3836

Epoch 71/179
----------
train Loss: 1.6973 Acc: 0.5466
val Loss: 2.2389 Acc: 0.5205

Epoch 72/179
----------
train Loss: 1.6390 Acc: 0.5489
val Loss: 3.0435 Acc: 0.4155

Epoch 73/179
----------
train Loss: 1.6269 Acc: 0.5506
val Loss: 2.3442 Acc: 0.4406

Epoch 74/179
----------
train Loss: 1.5927 Acc: 0.5606
val Loss: 2.5748 Acc: 0.4475

Epoch 75/179
----------
train Loss: 1.6002 Acc: 0.5707
val Loss: 2.8327 Acc: 0.4201

Epoch 76/179
----------
train Loss: 1.5550 Acc: 0.5767
val Loss: 2.9531 Acc: 0.3995

Epoch 77/179
----------
train Loss: 1.5329 Acc: 0.5798
val Loss: 2.4193 Acc: 0.4680

Epoch 78/179
----------
train Loss: 1.5382 Acc: 0.5787
val Loss: 3.6864 Acc: 0.3721

Epoch 79/179
----------
train Loss: 1.5597 Acc: 0.5810
val Loss: 2.6065 Acc: 0.4772

Epoch 80/179
----------
train Loss: 1.5665 Acc: 0.5752
val Loss: 2.7153 Acc: 0.4566

Epoch 81/179
----------
train Loss: 1.4650 Acc: 0.5964
val Loss: 3.0431 Acc: 0.4087

Epoch 82/179
----------
train Loss: 1.4342 Acc: 0.6036
val Loss: 3.0572 Acc: 0.3790

Epoch 83/179
----------
train Loss: 1.4068 Acc: 0.6088
val Loss: 10.8570 Acc: 0.2192

Epoch 84/179
----------
train Loss: 1.4317 Acc: 0.6111
val Loss: 3.1956 Acc: 0.4566

Epoch 85/179
----------
train Loss: 1.3951 Acc: 0.6177
val Loss: 3.3790 Acc: 0.4087

Epoch 86/179
----------
train Loss: 1.3779 Acc: 0.6113
val Loss: 2.5244 Acc: 0.5091

Epoch 87/179
----------
train Loss: 1.3565 Acc: 0.6245
val Loss: 2.1392 Acc: 0.5571

Epoch 88/179
----------
train Loss: 1.3654 Acc: 0.6314
val Loss: 2.8801 Acc: 0.4977

Epoch 89/179
----------
train Loss: 1.3229 Acc: 0.6394
val Loss: 2.8548 Acc: 0.4863

Epoch 90/179
----------
train Loss: 1.3005 Acc: 0.6469
val Loss: 2.6894 Acc: 0.5023

Epoch 91/179
----------
train Loss: 1.2987 Acc: 0.6409
val Loss: 2.3965 Acc: 0.5251

Epoch 92/179
----------
train Loss: 1.2292 Acc: 0.6606
val Loss: 2.6739 Acc: 0.5183

Epoch 93/179
----------
train Loss: 1.2503 Acc: 0.6543
val Loss: 2.5912 Acc: 0.4840

Epoch 94/179
----------
train Loss: 1.2095 Acc: 0.6675
val Loss: 2.4954 Acc: 0.5320

Epoch 95/179
----------
train Loss: 1.2412 Acc: 0.6569
val Loss: 2.6446 Acc: 0.5068

Epoch 96/179
----------
train Loss: 1.1499 Acc: 0.6830
val Loss: 3.1656 Acc: 0.4566

Epoch 97/179
----------
train Loss: 1.1918 Acc: 0.6767
val Loss: 3.1829 Acc: 0.4589

Epoch 98/179
----------
train Loss: 1.1612 Acc: 0.6856
val Loss: 2.9888 Acc: 0.4475

Epoch 99/179
----------
train Loss: 1.1046 Acc: 0.6925
val Loss: 2.9855 Acc: 0.4543

Epoch 100/179
----------
train Loss: 1.1626 Acc: 0.6784
val Loss: 3.0056 Acc: 0.4977

Epoch 101/179
----------
train Loss: 1.1283 Acc: 0.6816
val Loss: 2.5652 Acc: 0.4863

Epoch 102/179
----------
train Loss: 1.1171 Acc: 0.6884
val Loss: 3.1670 Acc: 0.4680

Epoch 103/179
----------
train Loss: 1.1203 Acc: 0.6953
val Loss: 3.1607 Acc: 0.5251

Epoch 104/179
----------
train Loss: 1.0902 Acc: 0.6991
val Loss: 3.4468 Acc: 0.4817

Epoch 105/179
----------
train Loss: 1.0635 Acc: 0.6950
val Loss: 3.7231 Acc: 0.5091

Epoch 106/179
----------
train Loss: 1.0279 Acc: 0.7105
val Loss: 3.6550 Acc: 0.3470

Epoch 107/179
----------
train Loss: 1.0444 Acc: 0.7048
val Loss: 2.5643 Acc: 0.5228

Epoch 108/179
----------
train Loss: 1.0508 Acc: 0.7079
val Loss: 3.9264 Acc: 0.4361

Epoch 109/179
----------
train Loss: 1.0546 Acc: 0.6982
val Loss: 3.2733 Acc: 0.4635

Epoch 110/179
----------
train Loss: 0.9898 Acc: 0.7240
val Loss: 2.4743 Acc: 0.5388

Epoch 111/179
----------
train Loss: 0.9478 Acc: 0.7349
val Loss: 3.5492 Acc: 0.4041

Epoch 112/179
----------
train Loss: 0.9960 Acc: 0.7208
val Loss: 2.6232 Acc: 0.5320

Epoch 113/179
----------
train Loss: 0.9775 Acc: 0.7297
val Loss: 2.7480 Acc: 0.5091

Epoch 114/179
----------
train Loss: 0.9422 Acc: 0.7343
val Loss: 2.4831 Acc: 0.5662

Epoch 115/179
----------
train Loss: 0.9733 Acc: 0.7254
val Loss: 3.3589 Acc: 0.4612

Epoch 116/179
----------
train Loss: 0.9642 Acc: 0.7363
val Loss: 2.9614 Acc: 0.5525

Epoch 117/179
----------
train Loss: 0.9739 Acc: 0.7277
val Loss: 3.3160 Acc: 0.4429

Epoch 118/179
----------
train Loss: 0.9040 Acc: 0.7466
val Loss: 4.1396 Acc: 0.4658

Epoch 119/179
----------
train Loss: 0.9495 Acc: 0.7303
val Loss: 4.0093 Acc: 0.4064

Epoch 120/179
----------
train Loss: 0.8837 Acc: 0.7538
val Loss: 4.0680 Acc: 0.4384

Epoch 121/179
----------
train Loss: 0.9082 Acc: 0.7423
val Loss: 3.1367 Acc: 0.5457

Epoch 122/179
----------
train Loss: 0.8292 Acc: 0.7621
val Loss: 2.4526 Acc: 0.5594

Epoch 123/179
----------
train Loss: 0.9280 Acc: 0.7452
val Loss: 3.9910 Acc: 0.4452

Epoch 124/179
----------
train Loss: 0.8644 Acc: 0.7527
val Loss: 2.8038 Acc: 0.5594

Epoch 125/179
----------
train Loss: 0.8689 Acc: 0.7555
val Loss: 2.9996 Acc: 0.5068

Epoch 126/179
----------
train Loss: 0.8463 Acc: 0.7595
val Loss: 12.7266 Acc: 0.2626

Epoch 127/179
----------
train Loss: 0.8452 Acc: 0.7630
val Loss: 2.9372 Acc: 0.5000

Epoch 128/179
----------
train Loss: 0.8515 Acc: 0.7687
val Loss: 2.9844 Acc: 0.5068

Epoch 129/179
----------
train Loss: 0.8561 Acc: 0.7644
val Loss: 2.1947 Acc: 0.5868

Epoch 130/179
----------
train Loss: 0.8141 Acc: 0.7710
val Loss: 2.5511 Acc: 0.5457

Epoch 131/179
----------
train Loss: 0.8041 Acc: 0.7805
val Loss: 3.0824 Acc: 0.5160

Epoch 132/179
----------
train Loss: 0.8201 Acc: 0.7655
val Loss: 19.1734 Acc: 0.2922

Epoch 133/179
----------
train Loss: 0.7885 Acc: 0.7693
val Loss: 18.3794 Acc: 0.2055

Epoch 134/179
----------
train Loss: 0.8236 Acc: 0.7716
val Loss: 3.3261 Acc: 0.4543

Epoch 135/179
----------
train Loss: 0.7935 Acc: 0.7776
val Loss: 3.1798 Acc: 0.4909

Epoch 136/179
----------
train Loss: 0.7940 Acc: 0.7770
val Loss: 3.1723 Acc: 0.5000

Epoch 137/179
----------
train Loss: 0.7792 Acc: 0.7842
val Loss: 2.9939 Acc: 0.5525

Epoch 138/179
----------
train Loss: 0.7913 Acc: 0.7819
val Loss: 3.5050 Acc: 0.5000

Epoch 139/179
----------
train Loss: 0.7864 Acc: 0.7839
val Loss: 3.1742 Acc: 0.5434

Epoch 140/179
----------
train Loss: 0.7617 Acc: 0.7793
val Loss: 3.1436 Acc: 0.5023

Epoch 141/179
----------
train Loss: 0.7391 Acc: 0.7982
val Loss: 3.5360 Acc: 0.4817

Epoch 142/179
----------
train Loss: 0.7861 Acc: 0.7750
val Loss: 3.2330 Acc: 0.5091

Epoch 143/179
----------
train Loss: 0.7528 Acc: 0.7893
val Loss: 3.2284 Acc: 0.4795

Epoch 144/179
----------
train Loss: 0.7253 Acc: 0.8002
val Loss: 2.6224 Acc: 0.5434

Epoch 145/179
----------
train Loss: 0.7138 Acc: 0.7974
val Loss: 3.4331 Acc: 0.5068

Epoch 146/179
----------
train Loss: 0.7138 Acc: 0.8019
val Loss: 3.6105 Acc: 0.3858

Epoch 147/179
----------
train Loss: 0.7053 Acc: 0.8111
val Loss: 6.6283 Acc: 0.4429

Epoch 148/179
----------
train Loss: 0.7271 Acc: 0.7951
val Loss: 5.5305 Acc: 0.4315

Epoch 149/179
----------
train Loss: 0.6840 Acc: 0.8045
val Loss: 4.1965 Acc: 0.5434

Epoch 150/179
----------
train Loss: 0.7067 Acc: 0.8040
val Loss: 3.7412 Acc: 0.4795

Epoch 151/179
----------
train Loss: 0.6769 Acc: 0.8077
val Loss: 2.9477 Acc: 0.4954

Epoch 152/179
----------
train Loss: 0.6993 Acc: 0.8054
val Loss: 4.4066 Acc: 0.4406

Epoch 153/179
----------
train Loss: 0.7103 Acc: 0.8042
val Loss: 3.5903 Acc: 0.4361

Epoch 154/179
----------
train Loss: 0.6632 Acc: 0.8169
val Loss: 3.1965 Acc: 0.5342

Epoch 155/179
----------
train Loss: 0.6353 Acc: 0.8191
val Loss: 2.8971 Acc: 0.5571

Epoch 156/179
----------
train Loss: 0.6708 Acc: 0.8126
val Loss: 3.2305 Acc: 0.4977

Epoch 157/179
----------
train Loss: 0.6564 Acc: 0.8071
val Loss: 3.1976 Acc: 0.5137

Epoch 158/179
----------
train Loss: 0.6793 Acc: 0.8085
val Loss: 6.3946 Acc: 0.4269

Epoch 159/179
----------
train Loss: 0.6480 Acc: 0.8160
val Loss: 3.1409 Acc: 0.5297

Epoch 160/179
----------
train Loss: 0.6936 Acc: 0.8040
val Loss: 3.6677 Acc: 0.4817

Epoch 161/179
----------
train Loss: 0.6730 Acc: 0.8105
val Loss: 2.4924 Acc: 0.5731

Epoch 162/179
----------
train Loss: 0.6585 Acc: 0.8120
val Loss: 2.9146 Acc: 0.5822

Epoch 163/179
----------
train Loss: 0.6128 Acc: 0.8223
val Loss: 4.6386 Acc: 0.4863

Epoch 164/179
----------
train Loss: 0.6221 Acc: 0.8286
val Loss: 3.9397 Acc: 0.4977

Epoch 165/179
----------
train Loss: 0.6088 Acc: 0.8255
val Loss: 4.1081 Acc: 0.4726

Epoch 166/179
----------
train Loss: 0.6054 Acc: 0.8272
val Loss: 4.9238 Acc: 0.4315

Epoch 167/179
----------
train Loss: 0.6070 Acc: 0.8260
val Loss: 3.7177 Acc: 0.5411

Epoch 168/179
----------
train Loss: 0.6051 Acc: 0.8263
val Loss: 3.2171 Acc: 0.5502

Epoch 169/179
----------
train Loss: 0.6493 Acc: 0.8234
val Loss: 3.6163 Acc: 0.5205

Epoch 170/179
----------
train Loss: 0.6293 Acc: 0.8206
val Loss: 3.1985 Acc: 0.5731

Epoch 171/179
----------
train Loss: 0.6242 Acc: 0.8220
val Loss: 3.3141 Acc: 0.5753

Epoch 172/179
----------
train Loss: 0.6161 Acc: 0.8194
val Loss: 4.8859 Acc: 0.3699

Epoch 173/179
----------
train Loss: 0.6226 Acc: 0.8220
val Loss: 2.7830 Acc: 0.5662

Epoch 174/179
----------
train Loss: 0.6322 Acc: 0.8243
val Loss: 2.9825 Acc: 0.5502

Epoch 175/179
----------
train Loss: 0.5913 Acc: 0.8338
val Loss: 3.5050 Acc: 0.5068

Epoch 176/179
----------
train Loss: 0.5866 Acc: 0.8335
val Loss: 5.6109 Acc: 0.3676

Epoch 177/179
----------
train Loss: 0.6027 Acc: 0.8332
val Loss: 3.2283 Acc: 0.5114

Epoch 178/179
----------
train Loss: 0.5603 Acc: 0.8426
val Loss: 7.0025 Acc: 0.4247

Epoch 179/179
----------
train Loss: 0.6123 Acc: 0.8300
val Loss: 4.2600 Acc: 0.4521

Training complete in 115m 34s
Best val Acc: 0.586758
creating directory:  /workspace/ruilei/hw/result/taskC_Adam_SGD_2019-04-15-21_56
