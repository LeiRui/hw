nohup: ignoring input
PyTorch Version:  1.0.1.post2
Torchvision Version:  0.2.2
Net(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)
  (fclass): Linear(in_features=2048, out_features=65, bias=True)
)
Initializing Datasets and Dataloaders...
Params to learn:
	 conv1.weight
	 bn1.weight
	 bn1.bias
	 layer1.0.conv1.weight
	 layer1.0.bn1.weight
	 layer1.0.bn1.bias
	 layer1.0.conv2.weight
	 layer1.0.bn2.weight
	 layer1.0.bn2.bias
	 layer1.0.conv3.weight
	 layer1.0.bn3.weight
	 layer1.0.bn3.bias
	 layer1.0.downsample.0.weight
	 layer1.0.downsample.1.weight
	 layer1.0.downsample.1.bias
	 layer1.1.conv1.weight
	 layer1.1.bn1.weight
	 layer1.1.bn1.bias
	 layer1.1.conv2.weight
	 layer1.1.bn2.weight
	 layer1.1.bn2.bias
	 layer1.1.conv3.weight
	 layer1.1.bn3.weight
	 layer1.1.bn3.bias
	 layer1.2.conv1.weight
	 layer1.2.bn1.weight
	 layer1.2.bn1.bias
	 layer1.2.conv2.weight
	 layer1.2.bn2.weight
	 layer1.2.bn2.bias
	 layer1.2.conv3.weight
	 layer1.2.bn3.weight
	 layer1.2.bn3.bias
	 layer2.0.conv1.weight
	 layer2.0.bn1.weight
	 layer2.0.bn1.bias
	 layer2.0.conv2.weight
	 layer2.0.bn2.weight
	 layer2.0.bn2.bias
	 layer2.0.conv3.weight
	 layer2.0.bn3.weight
	 layer2.0.bn3.bias
	 layer2.0.downsample.0.weight
	 layer2.0.downsample.1.weight
	 layer2.0.downsample.1.bias
	 layer2.1.conv1.weight
	 layer2.1.bn1.weight
	 layer2.1.bn1.bias
	 layer2.1.conv2.weight
	 layer2.1.bn2.weight
	 layer2.1.bn2.bias
	 layer2.1.conv3.weight
	 layer2.1.bn3.weight
	 layer2.1.bn3.bias
	 layer2.2.conv1.weight
	 layer2.2.bn1.weight
	 layer2.2.bn1.bias
	 layer2.2.conv2.weight
	 layer2.2.bn2.weight
	 layer2.2.bn2.bias
	 layer2.2.conv3.weight
	 layer2.2.bn3.weight
	 layer2.2.bn3.bias
	 layer3.0.conv1.weight
	 layer3.0.bn1.weight
	 layer3.0.bn1.bias
	 layer3.0.conv2.weight
	 layer3.0.bn2.weight
	 layer3.0.bn2.bias
	 layer3.0.conv3.weight
	 layer3.0.bn3.weight
	 layer3.0.bn3.bias
	 layer3.0.downsample.0.weight
	 layer3.0.downsample.1.weight
	 layer3.0.downsample.1.bias
	 layer3.1.conv1.weight
	 layer3.1.bn1.weight
	 layer3.1.bn1.bias
	 layer3.1.conv2.weight
	 layer3.1.bn2.weight
	 layer3.1.bn2.bias
	 layer3.1.conv3.weight
	 layer3.1.bn3.weight
	 layer3.1.bn3.bias
	 layer3.2.conv1.weight
	 layer3.2.bn1.weight
	 layer3.2.bn1.bias
	 layer3.2.conv2.weight
	 layer3.2.bn2.weight
	 layer3.2.bn2.bias
	 layer3.2.conv3.weight
	 layer3.2.bn3.weight
	 layer3.2.bn3.bias
	 layer4.0.conv1.weight
	 layer4.0.bn1.weight
	 layer4.0.bn1.bias
	 layer4.0.conv2.weight
	 layer4.0.bn2.weight
	 layer4.0.bn2.bias
	 layer4.0.conv3.weight
	 layer4.0.bn3.weight
	 layer4.0.bn3.bias
	 layer4.0.downsample.0.weight
	 layer4.0.downsample.1.weight
	 layer4.0.downsample.1.bias
	 layer4.1.conv1.weight
	 layer4.1.bn1.weight
	 layer4.1.bn1.bias
	 layer4.1.conv2.weight
	 layer4.1.bn2.weight
	 layer4.1.bn2.bias
	 layer4.1.conv3.weight
	 layer4.1.bn3.weight
	 layer4.1.bn3.bias
	 layer4.2.conv1.weight
	 layer4.2.bn1.weight
	 layer4.2.bn1.bias
	 layer4.2.conv2.weight
	 layer4.2.bn2.weight
	 layer4.2.bn2.bias
	 layer4.2.conv3.weight
	 layer4.2.bn3.weight
	 layer4.2.bn3.bias
	 fclass.weight
	 fclass.bias
Epoch 0/179
----------
train Loss: 4.8554 Acc: 0.0215
val Loss: 4.8036 Acc: 0.0320

Epoch 1/179
----------
train Loss: 4.8083 Acc: 0.0264
val Loss: 4.8608 Acc: 0.0365

Epoch 2/179
----------
train Loss: 4.7314 Acc: 0.0275
val Loss: 4.7630 Acc: 0.0160

Epoch 3/179
----------
train Loss: 4.7218 Acc: 0.0315
val Loss: 5.1928 Acc: 0.0228

Epoch 4/179
----------
train Loss: 4.7002 Acc: 0.0344
val Loss: 5.5875 Acc: 0.0274

Epoch 5/179
----------
train Loss: 4.6703 Acc: 0.0390
val Loss: 4.9086 Acc: 0.0388

Epoch 6/179
----------
train Loss: 4.6048 Acc: 0.0344
val Loss: 4.7653 Acc: 0.0320

Epoch 7/179
----------
train Loss: 4.5751 Acc: 0.0453
val Loss: 4.7644 Acc: 0.0251

Epoch 8/179
----------
train Loss: 4.5257 Acc: 0.0401
val Loss: 4.5091 Acc: 0.0731

Epoch 9/179
----------
train Loss: 4.5311 Acc: 0.0456
val Loss: 4.8808 Acc: 0.0502

Epoch 10/179
----------
train Loss: 4.4793 Acc: 0.0459
val Loss: 5.2993 Acc: 0.0457

Epoch 11/179
----------
train Loss: 4.4778 Acc: 0.0487
val Loss: 4.5218 Acc: 0.0525

Epoch 12/179
----------
train Loss: 4.4311 Acc: 0.0507
val Loss: 4.9580 Acc: 0.0434

Epoch 13/179
----------
train Loss: 4.4257 Acc: 0.0527
val Loss: 4.7755 Acc: 0.0639

Epoch 14/179
----------
train Loss: 4.3945 Acc: 0.0539
val Loss: 4.6511 Acc: 0.0479

Epoch 15/179
----------
train Loss: 4.3689 Acc: 0.0602
val Loss: 4.6510 Acc: 0.0571

Epoch 16/179
----------
train Loss: 4.3016 Acc: 0.0588
val Loss: 5.0688 Acc: 0.0616

Epoch 17/179
----------
train Loss: 4.3391 Acc: 0.0613
val Loss: 4.6700 Acc: 0.0639

Epoch 18/179
----------
train Loss: 4.3014 Acc: 0.0619
val Loss: 5.3281 Acc: 0.0799

Epoch 19/179
----------
train Loss: 4.2850 Acc: 0.0631
val Loss: 4.7617 Acc: 0.0616

Epoch 20/179
----------
train Loss: 4.2528 Acc: 0.0645
val Loss: 4.3037 Acc: 0.0616

Epoch 21/179
----------
train Loss: 4.2696 Acc: 0.0639
val Loss: 4.8906 Acc: 0.0616

Epoch 22/179
----------
train Loss: 4.2506 Acc: 0.0651
val Loss: 4.6166 Acc: 0.0571

Epoch 23/179
----------
train Loss: 4.2211 Acc: 0.0665
val Loss: 5.0591 Acc: 0.0616

Epoch 24/179
----------
train Loss: 4.2026 Acc: 0.0745
val Loss: 4.5412 Acc: 0.0708

Epoch 25/179
----------
train Loss: 4.2041 Acc: 0.0708
val Loss: 4.7359 Acc: 0.0731

Epoch 26/179
----------
train Loss: 4.1680 Acc: 0.0694
val Loss: 4.5912 Acc: 0.0776

Epoch 27/179
----------
train Loss: 4.1268 Acc: 0.0788
val Loss: 4.4568 Acc: 0.0708

Epoch 28/179
----------
train Loss: 4.1035 Acc: 0.0714
val Loss: 4.4967 Acc: 0.0868

Epoch 29/179
----------
train Loss: 4.1086 Acc: 0.0823
val Loss: 4.7573 Acc: 0.0662

Epoch 30/179
----------
train Loss: 4.1081 Acc: 0.0817
val Loss: 4.6654 Acc: 0.0936

Epoch 31/179
----------
train Loss: 4.0822 Acc: 0.0823
val Loss: 4.4076 Acc: 0.0913

Epoch 32/179
----------
train Loss: 4.0183 Acc: 0.0880
val Loss: 4.3728 Acc: 0.0868

Epoch 33/179
----------
train Loss: 4.0448 Acc: 0.0851
val Loss: 4.6287 Acc: 0.0845

Epoch 34/179
----------
train Loss: 4.0337 Acc: 0.0868
val Loss: 4.5305 Acc: 0.0799

Epoch 35/179
----------
train Loss: 4.0259 Acc: 0.0871
val Loss: 5.0770 Acc: 0.0731

Epoch 36/179
----------
train Loss: 3.9983 Acc: 0.0854
val Loss: 4.5353 Acc: 0.0913

Epoch 37/179
----------
train Loss: 4.0002 Acc: 0.0911
val Loss: 4.6368 Acc: 0.1096

Epoch 38/179
----------
train Loss: 3.9806 Acc: 0.0929
val Loss: 4.5871 Acc: 0.1142

Epoch 39/179
----------
train Loss: 3.9839 Acc: 0.0866
val Loss: 6.2740 Acc: 0.0685

Epoch 40/179
----------
train Loss: 3.9346 Acc: 0.0923
val Loss: 5.0898 Acc: 0.0959

Epoch 41/179
----------
train Loss: 3.9237 Acc: 0.0966
val Loss: 4.1925 Acc: 0.0868

Epoch 42/179
----------
train Loss: 3.8973 Acc: 0.1000
val Loss: 4.6091 Acc: 0.0913

Epoch 43/179
----------
train Loss: 3.9123 Acc: 0.1003
val Loss: 4.7682 Acc: 0.1027

Epoch 44/179
----------
train Loss: 3.8984 Acc: 0.0954
val Loss: 4.7205 Acc: 0.1164

Epoch 45/179
----------
train Loss: 3.8674 Acc: 0.1012
val Loss: 4.6399 Acc: 0.0776

Epoch 46/179
----------
train Loss: 3.8815 Acc: 0.0997
val Loss: 4.8424 Acc: 0.0959

Epoch 47/179
----------
train Loss: 3.8816 Acc: 0.1038
val Loss: 4.9175 Acc: 0.0982

Epoch 48/179
----------
train Loss: 3.8890 Acc: 0.1035
val Loss: 4.3446 Acc: 0.0982

Epoch 49/179
----------
train Loss: 3.8438 Acc: 0.1101
val Loss: 4.2514 Acc: 0.1233

Epoch 50/179
----------
train Loss: 3.8320 Acc: 0.1038
val Loss: 4.3441 Acc: 0.1096

Epoch 51/179
----------
train Loss: 3.8496 Acc: 0.0986
val Loss: 4.7354 Acc: 0.1027

Epoch 52/179
----------
train Loss: 3.8169 Acc: 0.1118
val Loss: 4.4660 Acc: 0.0959

Epoch 53/179
----------
train Loss: 3.8192 Acc: 0.1146
val Loss: 4.3801 Acc: 0.1096

Epoch 54/179
----------
train Loss: 3.8205 Acc: 0.1115
val Loss: 4.3770 Acc: 0.1187

Epoch 55/179
----------
train Loss: 3.7670 Acc: 0.1126
val Loss: 4.5415 Acc: 0.1096

Epoch 56/179
----------
train Loss: 3.7768 Acc: 0.1109
val Loss: 4.0559 Acc: 0.1370

Epoch 57/179
----------
train Loss: 3.7797 Acc: 0.1126
val Loss: 4.1188 Acc: 0.1324

Epoch 58/179
----------
train Loss: 3.7266 Acc: 0.1204
val Loss: 4.2071 Acc: 0.1279

Epoch 59/179
----------
train Loss: 3.7392 Acc: 0.1238
val Loss: 4.2280 Acc: 0.1164

Epoch 60/179
----------
train Loss: 3.7116 Acc: 0.1230
val Loss: 4.3266 Acc: 0.1256

Epoch 61/179
----------
train Loss: 3.7248 Acc: 0.1201
val Loss: 4.7924 Acc: 0.1187

Epoch 62/179
----------
train Loss: 3.7043 Acc: 0.1261
val Loss: 4.2160 Acc: 0.1256

Epoch 63/179
----------
train Loss: 3.6899 Acc: 0.1218
val Loss: 4.4138 Acc: 0.1142

Epoch 64/179
----------
train Loss: 3.7018 Acc: 0.1238
val Loss: 4.5074 Acc: 0.1233

Epoch 65/179
----------
train Loss: 3.7332 Acc: 0.1175
val Loss: 4.4796 Acc: 0.1301

Epoch 66/179
----------
train Loss: 3.6927 Acc: 0.1284
val Loss: 4.3762 Acc: 0.1279

Epoch 67/179
----------
train Loss: 3.6567 Acc: 0.1356
val Loss: 4.4818 Acc: 0.1256

Epoch 68/179
----------
train Loss: 3.6326 Acc: 0.1313
val Loss: 4.0875 Acc: 0.1484

Epoch 69/179
----------
train Loss: 3.6629 Acc: 0.1284
val Loss: 4.3741 Acc: 0.1233

Epoch 70/179
----------
train Loss: 3.6566 Acc: 0.1298
val Loss: 4.2018 Acc: 0.1438

Epoch 71/179
----------
train Loss: 3.6424 Acc: 0.1321
val Loss: 4.3033 Acc: 0.1301

Epoch 72/179
----------
train Loss: 3.6319 Acc: 0.1284
val Loss: 4.5577 Acc: 0.1393

Epoch 73/179
----------
train Loss: 3.6411 Acc: 0.1290
val Loss: 4.1244 Acc: 0.1416

Epoch 74/179
----------
train Loss: 3.6343 Acc: 0.1275
val Loss: 4.0301 Acc: 0.1324

Epoch 75/179
----------
train Loss: 3.5560 Acc: 0.1407
val Loss: 4.1355 Acc: 0.1256

Epoch 76/179
----------
train Loss: 3.6099 Acc: 0.1376
val Loss: 4.0033 Acc: 0.1370

Epoch 77/179
----------
train Loss: 3.5863 Acc: 0.1439
val Loss: 4.1877 Acc: 0.1279

Epoch 78/179
----------
train Loss: 3.5743 Acc: 0.1413
val Loss: 4.0491 Acc: 0.1370

Epoch 79/179
----------
train Loss: 3.5538 Acc: 0.1473
val Loss: 4.4707 Acc: 0.1301

Epoch 80/179
----------
train Loss: 3.5477 Acc: 0.1470
val Loss: 4.2156 Acc: 0.1210

Epoch 81/179
----------
train Loss: 3.5414 Acc: 0.1456
val Loss: 4.3721 Acc: 0.1416

Epoch 82/179
----------
train Loss: 3.5322 Acc: 0.1508
val Loss: 4.8952 Acc: 0.1416

Epoch 83/179
----------
train Loss: 3.5447 Acc: 0.1387
val Loss: 4.2629 Acc: 0.1575

Epoch 84/179
----------
train Loss: 3.4836 Acc: 0.1508
val Loss: 4.3948 Acc: 0.1256

Epoch 85/179
----------
train Loss: 3.5097 Acc: 0.1599
val Loss: 4.3483 Acc: 0.1438

Epoch 86/179
----------
train Loss: 3.4733 Acc: 0.1553
val Loss: 4.5593 Acc: 0.1461

Epoch 87/179
----------
train Loss: 3.4824 Acc: 0.1568
val Loss: 4.5112 Acc: 0.1553

Epoch 88/179
----------
train Loss: 3.4877 Acc: 0.1574
val Loss: 4.3991 Acc: 0.1461

Epoch 89/179
----------
train Loss: 3.4643 Acc: 0.1611
val Loss: 4.1195 Acc: 0.1256

Epoch 90/179
----------
train Loss: 3.5223 Acc: 0.1485
val Loss: 5.0814 Acc: 0.1553

Epoch 91/179
----------
train Loss: 3.4668 Acc: 0.1579
val Loss: 4.4348 Acc: 0.1575

Epoch 92/179
----------
train Loss: 3.4698 Acc: 0.1662
val Loss: 4.7177 Acc: 0.1461

Epoch 93/179
----------
train Loss: 3.4338 Acc: 0.1694
val Loss: 4.2715 Acc: 0.1484

Epoch 94/179
----------
train Loss: 3.4469 Acc: 0.1668
val Loss: 4.2012 Acc: 0.1438

Epoch 95/179
----------
train Loss: 3.4413 Acc: 0.1508
val Loss: 4.2040 Acc: 0.1256

Epoch 96/179
----------
train Loss: 3.4500 Acc: 0.1645
val Loss: 4.1999 Acc: 0.1438

Epoch 97/179
----------
train Loss: 3.4092 Acc: 0.1777
val Loss: 4.0818 Acc: 0.1598

Epoch 98/179
----------
train Loss: 3.4040 Acc: 0.1697
val Loss: 4.2285 Acc: 0.1735

Epoch 99/179
----------
train Loss: 3.4355 Acc: 0.1645
val Loss: 4.4300 Acc: 0.1393

Epoch 100/179
----------
train Loss: 3.4321 Acc: 0.1531
val Loss: 4.7722 Acc: 0.1735

Epoch 101/179
----------
train Loss: 3.4184 Acc: 0.1665
val Loss: 4.4071 Acc: 0.1689

Epoch 102/179
----------
train Loss: 3.3984 Acc: 0.1734
val Loss: 4.2264 Acc: 0.1758

Epoch 103/179
----------
train Loss: 3.3503 Acc: 0.1811
val Loss: 4.1071 Acc: 0.1735

Epoch 104/179
----------
train Loss: 3.3833 Acc: 0.1740
val Loss: 4.4872 Acc: 0.1461

Epoch 105/179
----------
train Loss: 3.4321 Acc: 0.1766
val Loss: 3.9876 Acc: 0.1689

Epoch 106/179
----------
train Loss: 3.3510 Acc: 0.1800
val Loss: 3.9551 Acc: 0.1826

Epoch 107/179
----------
train Loss: 3.3475 Acc: 0.1811
val Loss: 4.1474 Acc: 0.1553

Epoch 108/179
----------
train Loss: 3.3861 Acc: 0.1763
val Loss: 4.2527 Acc: 0.1393

Epoch 109/179
----------
train Loss: 3.3475 Acc: 0.1763
val Loss: 4.1383 Acc: 0.1553

Epoch 110/179
----------
train Loss: 3.3564 Acc: 0.1731
val Loss: 4.1941 Acc: 0.1347

Epoch 111/179
----------
train Loss: 3.3070 Acc: 0.1803
val Loss: 4.1916 Acc: 0.1781

Epoch 112/179
----------
train Loss: 3.3448 Acc: 0.1743
val Loss: 4.2170 Acc: 0.1598

Epoch 113/179
----------
train Loss: 3.3204 Acc: 0.1843
val Loss: 4.1185 Acc: 0.1644

Epoch 114/179
----------
train Loss: 3.2762 Acc: 0.1917
val Loss: 4.0088 Acc: 0.1781

Epoch 115/179
----------
train Loss: 3.3056 Acc: 0.1852
val Loss: 4.3994 Acc: 0.1438

Epoch 116/179
----------
train Loss: 3.2768 Acc: 0.1960
val Loss: 3.9612 Acc: 0.1918

Epoch 117/179
----------
train Loss: 3.3023 Acc: 0.1903
val Loss: 4.3238 Acc: 0.1598

Epoch 118/179
----------
train Loss: 3.3074 Acc: 0.1846
val Loss: 4.7955 Acc: 0.1689

Epoch 119/179
----------
train Loss: 3.3179 Acc: 0.1826
val Loss: 4.0612 Acc: 0.1804

Epoch 120/179
----------
train Loss: 3.2435 Acc: 0.1972
val Loss: 4.1857 Acc: 0.1530

Epoch 121/179
----------
train Loss: 3.2770 Acc: 0.1958
val Loss: 3.9477 Acc: 0.1941

Epoch 122/179
----------
train Loss: 3.2673 Acc: 0.1929
val Loss: 4.6233 Acc: 0.1712

Epoch 123/179
----------
train Loss: 3.2658 Acc: 0.1958
val Loss: 3.9974 Acc: 0.1872

Epoch 124/179
----------
train Loss: 3.2394 Acc: 0.1955
val Loss: 3.8805 Acc: 0.1986

Epoch 125/179
----------
train Loss: 3.2612 Acc: 0.2044
val Loss: 4.1761 Acc: 0.1804

Epoch 126/179
----------
train Loss: 3.2278 Acc: 0.2052
val Loss: 4.0358 Acc: 0.1735

Epoch 127/179
----------
train Loss: 3.2612 Acc: 0.1903
val Loss: 3.9650 Acc: 0.1781

Epoch 128/179
----------
train Loss: 3.2259 Acc: 0.2003
val Loss: 4.2199 Acc: 0.1804

Epoch 129/179
----------
train Loss: 3.2294 Acc: 0.1963
val Loss: 4.1037 Acc: 0.1826

Epoch 130/179
----------
train Loss: 3.1990 Acc: 0.2029
val Loss: 4.0539 Acc: 0.1941

Epoch 131/179
----------
train Loss: 3.1724 Acc: 0.2095
val Loss: 3.9237 Acc: 0.1712

Epoch 132/179
----------
train Loss: 3.2016 Acc: 0.1972
val Loss: 3.9011 Acc: 0.1735

Epoch 133/179
----------
train Loss: 3.1385 Acc: 0.2112
val Loss: 4.2364 Acc: 0.1941

Epoch 134/179
----------
train Loss: 3.1789 Acc: 0.2089
val Loss: 3.8628 Acc: 0.1826

Epoch 135/179
----------
train Loss: 3.1806 Acc: 0.2032
val Loss: 4.0381 Acc: 0.1826

Epoch 136/179
----------
train Loss: 3.1666 Acc: 0.2184
val Loss: 4.1915 Acc: 0.1872

Epoch 137/179
----------
train Loss: 3.1932 Acc: 0.1986
val Loss: 4.1725 Acc: 0.1895

Epoch 138/179
----------
train Loss: 3.1834 Acc: 0.2147
val Loss: 4.1587 Acc: 0.1735

Epoch 139/179
----------
train Loss: 3.1709 Acc: 0.2164
val Loss: 4.1741 Acc: 0.1712

Epoch 140/179
----------
train Loss: 3.1568 Acc: 0.2127
val Loss: 4.2129 Acc: 0.1918

Epoch 141/179
----------
train Loss: 3.1183 Acc: 0.2279
val Loss: 3.8107 Acc: 0.1918

Epoch 142/179
----------
train Loss: 3.1407 Acc: 0.2307
val Loss: 4.5136 Acc: 0.1712

Epoch 143/179
----------
train Loss: 3.1456 Acc: 0.2279
val Loss: 4.4538 Acc: 0.1918

Epoch 144/179
----------
train Loss: 3.1311 Acc: 0.2193
val Loss: 3.9409 Acc: 0.1849

Epoch 145/179
----------
train Loss: 3.1171 Acc: 0.2187
val Loss: 3.9018 Acc: 0.1735

Epoch 146/179
----------
train Loss: 3.1259 Acc: 0.2167
val Loss: 4.0752 Acc: 0.1986

Epoch 147/179
----------
train Loss: 3.1092 Acc: 0.2227
val Loss: 4.1770 Acc: 0.1849

Epoch 148/179
----------
train Loss: 3.1449 Acc: 0.2227
val Loss: 3.8438 Acc: 0.1963

Epoch 149/179
----------
train Loss: 3.0789 Acc: 0.2296
val Loss: 3.8844 Acc: 0.2123

Epoch 150/179
----------
train Loss: 3.1388 Acc: 0.2210
val Loss: 3.7698 Acc: 0.2306

Epoch 151/179
----------
train Loss: 3.1021 Acc: 0.2313
val Loss: 4.4063 Acc: 0.1735

Epoch 152/179
----------
train Loss: 3.0881 Acc: 0.2204
val Loss: 3.9304 Acc: 0.2055

Epoch 153/179
----------
train Loss: 3.0875 Acc: 0.2267
val Loss: 4.5135 Acc: 0.2055

Epoch 154/179
----------
train Loss: 3.1084 Acc: 0.2109
val Loss: 3.9431 Acc: 0.2032

Epoch 155/179
----------
train Loss: 3.0940 Acc: 0.2333
val Loss: 3.8683 Acc: 0.2078

Epoch 156/179
----------
train Loss: 3.0962 Acc: 0.2230
val Loss: 3.9707 Acc: 0.2055

Epoch 157/179
----------
train Loss: 3.0590 Acc: 0.2310
val Loss: 3.9102 Acc: 0.2032

Epoch 158/179
----------
train Loss: 3.0405 Acc: 0.2316
val Loss: 4.0675 Acc: 0.2055

Epoch 159/179
----------
train Loss: 3.0428 Acc: 0.2327
val Loss: 4.1938 Acc: 0.1986

Epoch 160/179
----------
train Loss: 3.0341 Acc: 0.2370
val Loss: 4.2182 Acc: 0.2078

Epoch 161/179
----------
train Loss: 3.0821 Acc: 0.2236
val Loss: 4.5698 Acc: 0.2192

Epoch 162/179
----------
train Loss: 3.0754 Acc: 0.2390
val Loss: 3.9646 Acc: 0.2169

Epoch 163/179
----------
train Loss: 2.9992 Acc: 0.2514
val Loss: 3.9670 Acc: 0.2169

Epoch 164/179
----------
train Loss: 2.9929 Acc: 0.2445
val Loss: 4.1707 Acc: 0.1963

Epoch 165/179
----------
train Loss: 3.0490 Acc: 0.2413
val Loss: 3.9795 Acc: 0.2146

Epoch 166/179
----------
train Loss: 3.0320 Acc: 0.2482
val Loss: 4.5876 Acc: 0.2032

Epoch 167/179
----------
train Loss: 3.0246 Acc: 0.2442
val Loss: 4.2851 Acc: 0.1781

Epoch 168/179
----------
train Loss: 2.9914 Acc: 0.2445
val Loss: 3.8459 Acc: 0.2215

Epoch 169/179
----------
train Loss: 2.9971 Acc: 0.2482
val Loss: 4.1218 Acc: 0.1941

Epoch 170/179
----------
train Loss: 3.0163 Acc: 0.2511
val Loss: 3.8111 Acc: 0.2260

Epoch 171/179
----------
train Loss: 3.0063 Acc: 0.2453
val Loss: 4.2681 Acc: 0.1872

Epoch 172/179
----------
train Loss: 2.9889 Acc: 0.2388
val Loss: 3.7014 Acc: 0.2192

Epoch 173/179
----------
train Loss: 2.9503 Acc: 0.2522
val Loss: 4.0543 Acc: 0.2146

Epoch 174/179
----------
train Loss: 2.9922 Acc: 0.2456
val Loss: 3.6506 Acc: 0.2397

Epoch 175/179
----------
train Loss: 2.9448 Acc: 0.2528
val Loss: 3.8974 Acc: 0.2100

Epoch 176/179
----------
train Loss: 3.0059 Acc: 0.2410
val Loss: 3.8181 Acc: 0.2329

Epoch 177/179
----------
train Loss: 2.9431 Acc: 0.2600
val Loss: 3.9814 Acc: 0.2146

Epoch 178/179
----------
train Loss: 2.9557 Acc: 0.2531
val Loss: 4.4147 Acc: 0.2055

Epoch 179/179
----------
train Loss: 2.9636 Acc: 0.2548
val Loss: 4.1429 Acc: 0.2100

Training complete in 123m 12s
Best val Acc: 0.239726
creating directory:  /workspace/ruilei/hw/result/taskC_weight_init_2019-04-15-13_38
