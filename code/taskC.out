nohup: ignoring input
PyTorch Version:  1.0.1.post2
Torchvision Version:  0.2.2
Net(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)
  (fclass): Linear(in_features=2048, out_features=65, bias=True)
)
Initializing Datasets and Dataloaders...
Params to learn:
	 conv1.weight
	 bn1.weight
	 bn1.bias
	 layer1.0.conv1.weight
	 layer1.0.bn1.weight
	 layer1.0.bn1.bias
	 layer1.0.conv2.weight
	 layer1.0.bn2.weight
	 layer1.0.bn2.bias
	 layer1.0.conv3.weight
	 layer1.0.bn3.weight
	 layer1.0.bn3.bias
	 layer1.0.downsample.0.weight
	 layer1.0.downsample.1.weight
	 layer1.0.downsample.1.bias
	 layer1.1.conv1.weight
	 layer1.1.bn1.weight
	 layer1.1.bn1.bias
	 layer1.1.conv2.weight
	 layer1.1.bn2.weight
	 layer1.1.bn2.bias
	 layer1.1.conv3.weight
	 layer1.1.bn3.weight
	 layer1.1.bn3.bias
	 layer1.2.conv1.weight
	 layer1.2.bn1.weight
	 layer1.2.bn1.bias
	 layer1.2.conv2.weight
	 layer1.2.bn2.weight
	 layer1.2.bn2.bias
	 layer1.2.conv3.weight
	 layer1.2.bn3.weight
	 layer1.2.bn3.bias
	 layer2.0.conv1.weight
	 layer2.0.bn1.weight
	 layer2.0.bn1.bias
	 layer2.0.conv2.weight
	 layer2.0.bn2.weight
	 layer2.0.bn2.bias
	 layer2.0.conv3.weight
	 layer2.0.bn3.weight
	 layer2.0.bn3.bias
	 layer2.0.downsample.0.weight
	 layer2.0.downsample.1.weight
	 layer2.0.downsample.1.bias
	 layer2.1.conv1.weight
	 layer2.1.bn1.weight
	 layer2.1.bn1.bias
	 layer2.1.conv2.weight
	 layer2.1.bn2.weight
	 layer2.1.bn2.bias
	 layer2.1.conv3.weight
	 layer2.1.bn3.weight
	 layer2.1.bn3.bias
	 layer2.2.conv1.weight
	 layer2.2.bn1.weight
	 layer2.2.bn1.bias
	 layer2.2.conv2.weight
	 layer2.2.bn2.weight
	 layer2.2.bn2.bias
	 layer2.2.conv3.weight
	 layer2.2.bn3.weight
	 layer2.2.bn3.bias
	 layer3.0.conv1.weight
	 layer3.0.bn1.weight
	 layer3.0.bn1.bias
	 layer3.0.conv2.weight
	 layer3.0.bn2.weight
	 layer3.0.bn2.bias
	 layer3.0.conv3.weight
	 layer3.0.bn3.weight
	 layer3.0.bn3.bias
	 layer3.0.downsample.0.weight
	 layer3.0.downsample.1.weight
	 layer3.0.downsample.1.bias
	 layer3.1.conv1.weight
	 layer3.1.bn1.weight
	 layer3.1.bn1.bias
	 layer3.1.conv2.weight
	 layer3.1.bn2.weight
	 layer3.1.bn2.bias
	 layer3.1.conv3.weight
	 layer3.1.bn3.weight
	 layer3.1.bn3.bias
	 layer3.2.conv1.weight
	 layer3.2.bn1.weight
	 layer3.2.bn1.bias
	 layer3.2.conv2.weight
	 layer3.2.bn2.weight
	 layer3.2.bn2.bias
	 layer3.2.conv3.weight
	 layer3.2.bn3.weight
	 layer3.2.bn3.bias
	 layer4.0.conv1.weight
	 layer4.0.bn1.weight
	 layer4.0.bn1.bias
	 layer4.0.conv2.weight
	 layer4.0.bn2.weight
	 layer4.0.bn2.bias
	 layer4.0.conv3.weight
	 layer4.0.bn3.weight
	 layer4.0.bn3.bias
	 layer4.0.downsample.0.weight
	 layer4.0.downsample.1.weight
	 layer4.0.downsample.1.bias
	 layer4.1.conv1.weight
	 layer4.1.bn1.weight
	 layer4.1.bn1.bias
	 layer4.1.conv2.weight
	 layer4.1.bn2.weight
	 layer4.1.bn2.bias
	 layer4.1.conv3.weight
	 layer4.1.bn3.weight
	 layer4.1.bn3.bias
	 layer4.2.conv1.weight
	 layer4.2.bn1.weight
	 layer4.2.bn1.bias
	 layer4.2.conv2.weight
	 layer4.2.bn2.weight
	 layer4.2.bn2.bias
	 layer4.2.conv3.weight
	 layer4.2.bn3.weight
	 layer4.2.bn3.bias
	 fclass.weight
	 fclass.bias
Epoch 0/179
----------
train Loss: 4.7060 Acc: 0.0235
val Loss: 8.7490 Acc: 0.0434

Epoch 1/179
----------
train Loss: 4.3544 Acc: 0.0378
val Loss: 7.0343 Acc: 0.0342

Epoch 2/179
----------
train Loss: 4.1346 Acc: 0.0510
val Loss: 4.8354 Acc: 0.0616

Epoch 3/179
----------
train Loss: 4.0515 Acc: 0.0565
val Loss: 8.5894 Acc: 0.0571

Epoch 4/179
----------
train Loss: 3.9961 Acc: 0.0579
val Loss: 5.2586 Acc: 0.0685

Epoch 5/179
----------
train Loss: 3.9251 Acc: 0.0688
val Loss: 7.6760 Acc: 0.0685

Epoch 6/179
----------
train Loss: 3.8686 Acc: 0.0679
val Loss: 6.3335 Acc: 0.0776

Epoch 7/179
----------
train Loss: 3.8422 Acc: 0.0805
val Loss: 5.4993 Acc: 0.0731

Epoch 8/179
----------
train Loss: 3.7962 Acc: 0.0774
val Loss: 6.3355 Acc: 0.1096

Epoch 9/179
----------
train Loss: 3.7564 Acc: 0.0877
val Loss: 5.2657 Acc: 0.1096

Epoch 10/179
----------
train Loss: 3.7200 Acc: 0.0906
val Loss: 8.1423 Acc: 0.0776

Epoch 11/179
----------
train Loss: 3.6930 Acc: 0.1060
val Loss: 5.1918 Acc: 0.0753

Epoch 12/179
----------
train Loss: 3.6804 Acc: 0.1124
val Loss: 5.0799 Acc: 0.0868

Epoch 13/179
----------
train Loss: 3.6390 Acc: 0.1078
val Loss: 9.1164 Acc: 0.0982

Epoch 14/179
----------
train Loss: 3.6018 Acc: 0.1066
val Loss: 9.5145 Acc: 0.1301

Epoch 15/179
----------
train Loss: 3.5870 Acc: 0.1184
val Loss: 5.7494 Acc: 0.1233

Epoch 16/179
----------
train Loss: 3.5316 Acc: 0.1293
val Loss: 11.0041 Acc: 0.1050

Epoch 17/179
----------
train Loss: 3.4984 Acc: 0.1373
val Loss: 4.3548 Acc: 0.0890

Epoch 18/179
----------
train Loss: 3.4914 Acc: 0.1493
val Loss: 9.7228 Acc: 0.1438

Epoch 19/179
----------
train Loss: 3.4600 Acc: 0.1361
val Loss: 7.2912 Acc: 0.1324

Epoch 20/179
----------
train Loss: 3.4350 Acc: 0.1479
val Loss: 4.7415 Acc: 0.1279

Epoch 21/179
----------
train Loss: 3.4036 Acc: 0.1505
val Loss: 5.0392 Acc: 0.1553

Epoch 22/179
----------
train Loss: 3.3935 Acc: 0.1508
val Loss: 8.0745 Acc: 0.1553

Epoch 23/179
----------
train Loss: 3.3569 Acc: 0.1637
val Loss: 5.2922 Acc: 0.1484

Epoch 24/179
----------
train Loss: 3.3437 Acc: 0.1657
val Loss: 10.9276 Acc: 0.1712

Epoch 25/179
----------
train Loss: 3.2790 Acc: 0.1688
val Loss: 5.5487 Acc: 0.1644

Epoch 26/179
----------
train Loss: 3.2932 Acc: 0.1734
val Loss: 5.3386 Acc: 0.1895

Epoch 27/179
----------
train Loss: 3.2509 Acc: 0.1852
val Loss: 5.1486 Acc: 0.1712

Epoch 28/179
----------
train Loss: 3.2076 Acc: 0.1869
val Loss: 6.2450 Acc: 0.1849

Epoch 29/179
----------
train Loss: 3.1995 Acc: 0.1972
val Loss: 6.4086 Acc: 0.1804

Epoch 30/179
----------
train Loss: 3.2081 Acc: 0.1940
val Loss: 7.4234 Acc: 0.1963

Epoch 31/179
----------
train Loss: 3.1446 Acc: 0.2072
val Loss: 11.8715 Acc: 0.1644

Epoch 32/179
----------
train Loss: 3.1374 Acc: 0.2058
val Loss: 10.7388 Acc: 0.1986

Epoch 33/179
----------
train Loss: 3.1166 Acc: 0.2161
val Loss: 6.3314 Acc: 0.2489

Epoch 34/179
----------
train Loss: 3.0852 Acc: 0.2187
val Loss: 8.8859 Acc: 0.1781

Epoch 35/179
----------
train Loss: 3.0758 Acc: 0.2221
val Loss: 6.3002 Acc: 0.1644

Epoch 36/179
----------
train Loss: 3.0406 Acc: 0.2316
val Loss: 11.5330 Acc: 0.2055

Epoch 37/179
----------
train Loss: 3.0062 Acc: 0.2430
val Loss: 8.0669 Acc: 0.1986

Epoch 38/179
----------
train Loss: 2.9845 Acc: 0.2399
val Loss: 14.2145 Acc: 0.2329

Epoch 39/179
----------
train Loss: 2.9714 Acc: 0.2388
val Loss: 4.9142 Acc: 0.2283

Epoch 40/179
----------
train Loss: 2.9224 Acc: 0.2548
val Loss: 5.8976 Acc: 0.2352

Epoch 41/179
----------
train Loss: 2.9225 Acc: 0.2499
val Loss: 25.0725 Acc: 0.2603

Epoch 42/179
----------
train Loss: 2.9387 Acc: 0.2465
val Loss: 6.8845 Acc: 0.2397

Epoch 43/179
----------
train Loss: 2.9073 Acc: 0.2585
val Loss: 10.7131 Acc: 0.2580

Epoch 44/179
----------
train Loss: 2.8818 Acc: 0.2752
val Loss: 11.0762 Acc: 0.2900

Epoch 45/179
----------
train Loss: 2.8333 Acc: 0.2760
val Loss: 4.8229 Acc: 0.2968

Epoch 46/179
----------
train Loss: 2.7733 Acc: 0.2794
val Loss: 5.1575 Acc: 0.2603

Epoch 47/179
----------
train Loss: 2.7705 Acc: 0.2789
val Loss: 10.0338 Acc: 0.2557

Epoch 48/179
----------
train Loss: 2.7597 Acc: 0.2763
val Loss: 12.7037 Acc: 0.2694

Epoch 49/179
----------
train Loss: 2.7175 Acc: 0.3024
val Loss: 16.0262 Acc: 0.2922

Epoch 50/179
----------
train Loss: 2.7295 Acc: 0.2978
val Loss: 9.2110 Acc: 0.2489

Epoch 51/179
----------
train Loss: 2.7105 Acc: 0.2961
val Loss: 9.6238 Acc: 0.3151

Epoch 52/179
----------
train Loss: 2.6438 Acc: 0.3213
val Loss: 7.1988 Acc: 0.3379

Epoch 53/179
----------
train Loss: 2.6579 Acc: 0.3210
val Loss: 7.6554 Acc: 0.3037

Epoch 54/179
----------
train Loss: 2.6027 Acc: 0.3130
val Loss: 10.9483 Acc: 0.2557

Epoch 55/179
----------
train Loss: 2.5794 Acc: 0.3325
val Loss: 17.3317 Acc: 0.2260

Epoch 56/179
----------
train Loss: 2.5722 Acc: 0.3259
val Loss: 7.0665 Acc: 0.3059

Epoch 57/179
----------
train Loss: 2.5520 Acc: 0.3371
val Loss: 14.6448 Acc: 0.3037

Epoch 58/179
----------
train Loss: 2.5081 Acc: 0.3488
val Loss: 8.2559 Acc: 0.2831

Epoch 59/179
----------
train Loss: 2.4762 Acc: 0.3525
val Loss: 7.0468 Acc: 0.3470

Epoch 60/179
----------
train Loss: 2.4781 Acc: 0.3557
val Loss: 13.2017 Acc: 0.3219

Epoch 61/179
----------
train Loss: 2.4252 Acc: 0.3565
val Loss: 9.7053 Acc: 0.3402

Epoch 62/179
----------
train Loss: 2.4280 Acc: 0.3715
val Loss: 14.8798 Acc: 0.3105

Epoch 63/179
----------
train Loss: 2.3555 Acc: 0.3815
val Loss: 6.0026 Acc: 0.3379

Epoch 64/179
----------
train Loss: 2.3769 Acc: 0.3775
val Loss: 10.2879 Acc: 0.3584

Epoch 65/179
----------
train Loss: 2.3529 Acc: 0.3892
val Loss: 13.5261 Acc: 0.3288

Epoch 66/179
----------
train Loss: 2.3380 Acc: 0.3872
val Loss: 7.7438 Acc: 0.3333

Epoch 67/179
----------
train Loss: 2.2851 Acc: 0.3924
val Loss: 9.9468 Acc: 0.3288

Epoch 68/179
----------
train Loss: 2.3269 Acc: 0.3924
val Loss: 30.2625 Acc: 0.3653

Epoch 69/179
----------
train Loss: 2.2880 Acc: 0.3961
val Loss: 17.4734 Acc: 0.3425

Epoch 70/179
----------
train Loss: 2.2480 Acc: 0.4058
val Loss: 24.4973 Acc: 0.3447

Epoch 71/179
----------
train Loss: 2.3753 Acc: 0.3801
val Loss: 8.5537 Acc: 0.3699

Epoch 72/179
----------
train Loss: 2.3276 Acc: 0.3904
val Loss: 11.0569 Acc: 0.3881

Epoch 73/179
----------
train Loss: 2.2830 Acc: 0.3955
val Loss: 3.8845 Acc: 0.3447

Epoch 74/179
----------
train Loss: 2.1928 Acc: 0.4162
val Loss: 3.0184 Acc: 0.4132

Epoch 75/179
----------
train Loss: 2.1834 Acc: 0.4259
val Loss: 4.0338 Acc: 0.3881

Epoch 76/179
----------
train Loss: 2.1727 Acc: 0.4219
val Loss: 3.2153 Acc: 0.3950

Epoch 77/179
----------
train Loss: 2.1634 Acc: 0.4293
val Loss: 3.9299 Acc: 0.3813

Epoch 78/179
----------
train Loss: 2.1017 Acc: 0.4465
val Loss: 4.0948 Acc: 0.3562

Epoch 79/179
----------
train Loss: 2.1270 Acc: 0.4311
val Loss: 5.7420 Acc: 0.2991

Epoch 80/179
----------
train Loss: 2.1126 Acc: 0.4394
val Loss: 4.7893 Acc: 0.3973

Epoch 81/179
----------
train Loss: 2.0713 Acc: 0.4511
val Loss: 10.2726 Acc: 0.4224

Epoch 82/179
----------
train Loss: 2.0303 Acc: 0.4629
val Loss: 6.7560 Acc: 0.3174

Epoch 83/179
----------
train Loss: 2.0292 Acc: 0.4580
val Loss: 3.3678 Acc: 0.4132

Epoch 84/179
----------
train Loss: 1.9996 Acc: 0.4680
val Loss: 4.1754 Acc: 0.4064

Epoch 85/179
----------
train Loss: 1.9588 Acc: 0.4680
val Loss: 4.6834 Acc: 0.3927

Epoch 86/179
----------
train Loss: 1.9655 Acc: 0.4723
val Loss: 5.4908 Acc: 0.3653

Epoch 87/179
----------
train Loss: 1.9358 Acc: 0.4847
val Loss: 4.7593 Acc: 0.3950

Epoch 88/179
----------
train Loss: 1.8782 Acc: 0.5010
val Loss: 5.3652 Acc: 0.3607

Epoch 89/179
----------
train Loss: 1.8914 Acc: 0.4818
val Loss: 7.6455 Acc: 0.3904

Epoch 90/179
----------
train Loss: 1.8395 Acc: 0.4987
val Loss: 3.9755 Acc: 0.4087

Epoch 91/179
----------
train Loss: 1.8530 Acc: 0.5079
val Loss: 4.7972 Acc: 0.4018

Epoch 92/179
----------
train Loss: 1.8437 Acc: 0.4976
val Loss: 4.7635 Acc: 0.4224

Epoch 93/179
----------
train Loss: 1.8474 Acc: 0.5085
val Loss: 3.8242 Acc: 0.3995

Epoch 94/179
----------
train Loss: 1.8500 Acc: 0.4984
val Loss: 5.3477 Acc: 0.4361

Epoch 95/179
----------
train Loss: 1.7930 Acc: 0.5196
val Loss: 7.4409 Acc: 0.4224

Epoch 96/179
----------
train Loss: 1.7847 Acc: 0.5199
val Loss: 6.8334 Acc: 0.4224

Epoch 97/179
----------
train Loss: 1.7574 Acc: 0.5308
val Loss: 4.3758 Acc: 0.4292

Epoch 98/179
----------
train Loss: 1.7333 Acc: 0.5322
val Loss: 3.1291 Acc: 0.4269

Epoch 99/179
----------
train Loss: 1.6990 Acc: 0.5417
val Loss: 5.5592 Acc: 0.3881

Epoch 100/179
----------
train Loss: 1.6859 Acc: 0.5360
val Loss: 4.3194 Acc: 0.4178

Epoch 101/179
----------
train Loss: 1.7294 Acc: 0.5328
val Loss: 3.5383 Acc: 0.4338

Epoch 102/179
----------
train Loss: 1.6767 Acc: 0.5457
val Loss: 3.5647 Acc: 0.4361

Epoch 103/179
----------
train Loss: 1.6621 Acc: 0.5426
val Loss: 3.2752 Acc: 0.4589

Epoch 104/179
----------
train Loss: 1.6462 Acc: 0.5578
val Loss: 3.8006 Acc: 0.4315

Epoch 105/179
----------
train Loss: 1.5837 Acc: 0.5709
val Loss: 3.0349 Acc: 0.4635

Epoch 106/179
----------
train Loss: 1.5488 Acc: 0.5727
val Loss: 3.2123 Acc: 0.4178

Epoch 107/179
----------
train Loss: 1.5864 Acc: 0.5675
val Loss: 3.2005 Acc: 0.4224

Epoch 108/179
----------
train Loss: 1.5263 Acc: 0.5712
val Loss: 2.7720 Acc: 0.4680

Epoch 109/179
----------
train Loss: 1.5319 Acc: 0.5821
val Loss: 3.4364 Acc: 0.4863

Epoch 110/179
----------
train Loss: 1.4718 Acc: 0.6042
val Loss: 2.7978 Acc: 0.4977

Epoch 111/179
----------
train Loss: 1.5314 Acc: 0.5916
val Loss: 2.7634 Acc: 0.4954

Epoch 112/179
----------
train Loss: 1.5004 Acc: 0.5973
val Loss: 2.5854 Acc: 0.4909

Epoch 113/179
----------
train Loss: 1.5080 Acc: 0.5933
val Loss: 3.1370 Acc: 0.4361

Epoch 114/179
----------
train Loss: 1.4267 Acc: 0.6171
val Loss: 2.8936 Acc: 0.4429

Epoch 115/179
----------
train Loss: 1.4224 Acc: 0.6156
val Loss: 2.9289 Acc: 0.4726

Epoch 116/179
----------
train Loss: 1.4424 Acc: 0.6085
val Loss: 3.0191 Acc: 0.4886

Epoch 117/179
----------
train Loss: 1.4098 Acc: 0.6093
val Loss: 3.0890 Acc: 0.4726

Epoch 118/179
----------
train Loss: 1.3847 Acc: 0.6205
val Loss: 2.9682 Acc: 0.4498

Epoch 119/179
----------
train Loss: 1.3994 Acc: 0.6165
val Loss: 3.6577 Acc: 0.3927

Epoch 120/179
----------
train Loss: 1.4024 Acc: 0.6165
val Loss: 3.6896 Acc: 0.4452

Epoch 121/179
----------
train Loss: 1.3689 Acc: 0.6331
val Loss: 3.1033 Acc: 0.4498

Epoch 122/179
----------
train Loss: 1.3494 Acc: 0.6208
val Loss: 3.0915 Acc: 0.4566

Epoch 123/179
----------
train Loss: 1.3157 Acc: 0.6371
val Loss: 3.5584 Acc: 0.4018

Epoch 124/179
----------
train Loss: 1.3047 Acc: 0.6354
val Loss: 5.0080 Acc: 0.4498

Epoch 125/179
----------
train Loss: 1.3051 Acc: 0.6440
val Loss: 2.9318 Acc: 0.4795

Epoch 126/179
----------
train Loss: 1.3591 Acc: 0.6280
val Loss: 3.0066 Acc: 0.4932

Epoch 127/179
----------
train Loss: 1.3033 Acc: 0.6449
val Loss: 2.9903 Acc: 0.4886

Epoch 128/179
----------
train Loss: 1.2829 Acc: 0.6486
val Loss: 2.8783 Acc: 0.5091

Epoch 129/179
----------
train Loss: 1.2547 Acc: 0.6586
val Loss: 2.7918 Acc: 0.4863

Epoch 130/179
----------
train Loss: 1.2463 Acc: 0.6569
val Loss: 3.0899 Acc: 0.4795

Epoch 131/179
----------
train Loss: 1.2892 Acc: 0.6377
val Loss: 3.0654 Acc: 0.4658

Epoch 132/179
----------
train Loss: 1.2191 Acc: 0.6652
val Loss: 2.8724 Acc: 0.4954

Epoch 133/179
----------
train Loss: 1.2141 Acc: 0.6733
val Loss: 2.4987 Acc: 0.5228

Epoch 134/179
----------
train Loss: 1.2009 Acc: 0.6644
val Loss: 2.9588 Acc: 0.5068

Epoch 135/179
----------
train Loss: 1.1716 Acc: 0.6695
val Loss: 2.7915 Acc: 0.5228

Epoch 136/179
----------
train Loss: 1.1796 Acc: 0.6750
val Loss: 4.0137 Acc: 0.3950

Epoch 137/179
----------
train Loss: 1.1583 Acc: 0.6741
val Loss: 3.0461 Acc: 0.4749

Epoch 138/179
----------
train Loss: 1.1050 Acc: 0.6991
val Loss: 3.0660 Acc: 0.4863

Epoch 139/179
----------
train Loss: 1.1644 Acc: 0.6770
val Loss: 2.9806 Acc: 0.4954

Epoch 140/179
----------
train Loss: 1.1252 Acc: 0.6910
val Loss: 3.2467 Acc: 0.4429

Epoch 141/179
----------
train Loss: 1.1193 Acc: 0.6973
val Loss: 3.3369 Acc: 0.4909

Epoch 142/179
----------
train Loss: 1.1228 Acc: 0.6922
val Loss: 2.6111 Acc: 0.5114

Epoch 143/179
----------
train Loss: 1.1623 Acc: 0.6862
val Loss: 2.7528 Acc: 0.5000

Epoch 144/179
----------
train Loss: 1.0872 Acc: 0.7005
val Loss: 3.0300 Acc: 0.5091

Epoch 145/179
----------
train Loss: 1.0326 Acc: 0.7177
val Loss: 2.7726 Acc: 0.5091

Epoch 146/179
----------
train Loss: 1.0698 Acc: 0.6933
val Loss: 2.9938 Acc: 0.4954

Epoch 147/179
----------
train Loss: 1.0415 Acc: 0.7160
val Loss: 3.0918 Acc: 0.4817

Epoch 148/179
----------
train Loss: 1.0657 Acc: 0.7031
val Loss: 3.0082 Acc: 0.4886

Epoch 149/179
----------
train Loss: 1.0557 Acc: 0.7214
val Loss: 3.1109 Acc: 0.4932

Epoch 150/179
----------
train Loss: 1.0788 Acc: 0.7031
val Loss: 2.6090 Acc: 0.5411

Epoch 151/179
----------
train Loss: 1.0446 Acc: 0.7140
val Loss: 3.0047 Acc: 0.4612

Epoch 152/179
----------
train Loss: 1.0270 Acc: 0.7168
val Loss: 2.7682 Acc: 0.5228

Epoch 153/179
----------
train Loss: 1.0184 Acc: 0.7223
val Loss: 3.2226 Acc: 0.4680

Epoch 154/179
----------
train Loss: 1.0029 Acc: 0.7291
val Loss: 3.2575 Acc: 0.4589

Epoch 155/179
----------
train Loss: 0.9928 Acc: 0.7291
val Loss: 2.9106 Acc: 0.5000

Epoch 156/179
----------
train Loss: 0.9722 Acc: 0.7386
val Loss: 3.0463 Acc: 0.5091

Epoch 157/179
----------
train Loss: 0.9533 Acc: 0.7323
val Loss: 3.1367 Acc: 0.4840

Epoch 158/179
----------
train Loss: 0.9857 Acc: 0.7306
val Loss: 2.9596 Acc: 0.5137

Epoch 159/179
----------
train Loss: 0.9433 Acc: 0.7412
val Loss: 3.1519 Acc: 0.4863

Epoch 160/179
----------
train Loss: 0.9272 Acc: 0.7340
val Loss: 3.1051 Acc: 0.4977

Epoch 161/179
----------
train Loss: 0.9961 Acc: 0.7291
val Loss: 2.8364 Acc: 0.5137

Epoch 162/179
----------
train Loss: 0.9213 Acc: 0.7406
val Loss: 3.0132 Acc: 0.5023

Epoch 163/179
----------
train Loss: 0.9350 Acc: 0.7412
val Loss: 2.6695 Acc: 0.5342

Epoch 164/179
----------
train Loss: 0.9495 Acc: 0.7372
val Loss: 3.1991 Acc: 0.5251

Epoch 165/179
----------
train Loss: 0.8775 Acc: 0.7509
val Loss: 3.1261 Acc: 0.4817

Epoch 166/179
----------
train Loss: 0.8575 Acc: 0.7524
val Loss: 3.2977 Acc: 0.4635

Epoch 167/179
----------
train Loss: 0.9578 Acc: 0.7366
val Loss: 2.7410 Acc: 0.5297

Epoch 168/179
----------
train Loss: 0.8837 Acc: 0.7633
val Loss: 2.8763 Acc: 0.5662

Epoch 169/179
----------
train Loss: 0.8925 Acc: 0.7584
val Loss: 3.7910 Acc: 0.4338

Epoch 170/179
----------
train Loss: 0.9160 Acc: 0.7544
val Loss: 3.0105 Acc: 0.5068

Epoch 171/179
----------
train Loss: 0.8635 Acc: 0.7584
val Loss: 3.1315 Acc: 0.5251

Epoch 172/179
----------
train Loss: 0.8635 Acc: 0.7587
val Loss: 3.6333 Acc: 0.4475

Epoch 173/179
----------
train Loss: 0.8569 Acc: 0.7681
val Loss: 2.9672 Acc: 0.5411

Epoch 174/179
----------
train Loss: 0.8796 Acc: 0.7549
val Loss: 2.8386 Acc: 0.5434

Epoch 175/179
----------
train Loss: 0.8142 Acc: 0.7641
val Loss: 2.8220 Acc: 0.5411

Epoch 176/179
----------
train Loss: 0.8023 Acc: 0.7707
val Loss: 2.8205 Acc: 0.5251

Epoch 177/179
----------
train Loss: 0.8822 Acc: 0.7564
val Loss: 3.0645 Acc: 0.5000

Epoch 178/179
----------
train Loss: 0.8419 Acc: 0.7612
val Loss: 3.0141 Acc: 0.5046

Epoch 179/179
----------
train Loss: 0.8489 Acc: 0.7653
val Loss: 4.6087 Acc: 0.4110

Training complete in 112m 37s
Best val Acc: 0.566210
creating directory:  /workspace/ruilei/hw/result/taskC_2019-04-15-13_18
