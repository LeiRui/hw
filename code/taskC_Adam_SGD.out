nohup: ignoring input
PyTorch Version:  1.0.1.post2
Torchvision Version:  0.2.2
Net(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)
  (fclass): Linear(in_features=2048, out_features=65, bias=True)
)
Initializing Datasets and Dataloaders...
Params to learn:
	 conv1.weight
	 bn1.weight
	 bn1.bias
	 layer1.0.conv1.weight
	 layer1.0.bn1.weight
	 layer1.0.bn1.bias
	 layer1.0.conv2.weight
	 layer1.0.bn2.weight
	 layer1.0.bn2.bias
	 layer1.0.conv3.weight
	 layer1.0.bn3.weight
	 layer1.0.bn3.bias
	 layer1.0.downsample.0.weight
	 layer1.0.downsample.1.weight
	 layer1.0.downsample.1.bias
	 layer1.1.conv1.weight
	 layer1.1.bn1.weight
	 layer1.1.bn1.bias
	 layer1.1.conv2.weight
	 layer1.1.bn2.weight
	 layer1.1.bn2.bias
	 layer1.1.conv3.weight
	 layer1.1.bn3.weight
	 layer1.1.bn3.bias
	 layer1.2.conv1.weight
	 layer1.2.bn1.weight
	 layer1.2.bn1.bias
	 layer1.2.conv2.weight
	 layer1.2.bn2.weight
	 layer1.2.bn2.bias
	 layer1.2.conv3.weight
	 layer1.2.bn3.weight
	 layer1.2.bn3.bias
	 layer2.0.conv1.weight
	 layer2.0.bn1.weight
	 layer2.0.bn1.bias
	 layer2.0.conv2.weight
	 layer2.0.bn2.weight
	 layer2.0.bn2.bias
	 layer2.0.conv3.weight
	 layer2.0.bn3.weight
	 layer2.0.bn3.bias
	 layer2.0.downsample.0.weight
	 layer2.0.downsample.1.weight
	 layer2.0.downsample.1.bias
	 layer2.1.conv1.weight
	 layer2.1.bn1.weight
	 layer2.1.bn1.bias
	 layer2.1.conv2.weight
	 layer2.1.bn2.weight
	 layer2.1.bn2.bias
	 layer2.1.conv3.weight
	 layer2.1.bn3.weight
	 layer2.1.bn3.bias
	 layer2.2.conv1.weight
	 layer2.2.bn1.weight
	 layer2.2.bn1.bias
	 layer2.2.conv2.weight
	 layer2.2.bn2.weight
	 layer2.2.bn2.bias
	 layer2.2.conv3.weight
	 layer2.2.bn3.weight
	 layer2.2.bn3.bias
	 layer3.0.conv1.weight
	 layer3.0.bn1.weight
	 layer3.0.bn1.bias
	 layer3.0.conv2.weight
	 layer3.0.bn2.weight
	 layer3.0.bn2.bias
	 layer3.0.conv3.weight
	 layer3.0.bn3.weight
	 layer3.0.bn3.bias
	 layer3.0.downsample.0.weight
	 layer3.0.downsample.1.weight
	 layer3.0.downsample.1.bias
	 layer3.1.conv1.weight
	 layer3.1.bn1.weight
	 layer3.1.bn1.bias
	 layer3.1.conv2.weight
	 layer3.1.bn2.weight
	 layer3.1.bn2.bias
	 layer3.1.conv3.weight
	 layer3.1.bn3.weight
	 layer3.1.bn3.bias
	 layer3.2.conv1.weight
	 layer3.2.bn1.weight
	 layer3.2.bn1.bias
	 layer3.2.conv2.weight
	 layer3.2.bn2.weight
	 layer3.2.bn2.bias
	 layer3.2.conv3.weight
	 layer3.2.bn3.weight
	 layer3.2.bn3.bias
	 layer4.0.conv1.weight
	 layer4.0.bn1.weight
	 layer4.0.bn1.bias
	 layer4.0.conv2.weight
	 layer4.0.bn2.weight
	 layer4.0.bn2.bias
	 layer4.0.conv3.weight
	 layer4.0.bn3.weight
	 layer4.0.bn3.bias
	 layer4.0.downsample.0.weight
	 layer4.0.downsample.1.weight
	 layer4.0.downsample.1.bias
	 layer4.1.conv1.weight
	 layer4.1.bn1.weight
	 layer4.1.bn1.bias
	 layer4.1.conv2.weight
	 layer4.1.bn2.weight
	 layer4.1.bn2.bias
	 layer4.1.conv3.weight
	 layer4.1.bn3.weight
	 layer4.1.bn3.bias
	 layer4.2.conv1.weight
	 layer4.2.bn1.weight
	 layer4.2.bn1.bias
	 layer4.2.conv2.weight
	 layer4.2.bn2.weight
	 layer4.2.bn2.bias
	 layer4.2.conv3.weight
	 layer4.2.bn3.weight
	 layer4.2.bn3.bias
	 fclass.weight
	 fclass.bias
Epoch 0/179
----------
train Loss: 4.4344 Acc: 0.0261
val Loss: 7.6873 Acc: 0.0068

Epoch 1/179
----------
train Loss: 4.1285 Acc: 0.0347
val Loss: 5.5745 Acc: 0.0388

Epoch 2/179
----------
train Loss: 4.0626 Acc: 0.0413
val Loss: 3.9491 Acc: 0.0479

Epoch 3/179
----------
train Loss: 3.9998 Acc: 0.0456
val Loss: 3.9032 Acc: 0.0502

Epoch 4/179
----------
train Loss: 3.9552 Acc: 0.0516
val Loss: 3.8911 Acc: 0.0571

Epoch 5/179
----------
train Loss: 3.9037 Acc: 0.0559
val Loss: 3.7956 Acc: 0.0708

Epoch 6/179
----------
train Loss: 3.8663 Acc: 0.0668
val Loss: 3.7726 Acc: 0.0685

Epoch 7/179
----------
train Loss: 3.7936 Acc: 0.0760
val Loss: 3.8061 Acc: 0.0502

Epoch 8/179
----------
train Loss: 3.7869 Acc: 0.0771
val Loss: 3.7525 Acc: 0.0662

Epoch 9/179
----------
train Loss: 3.7759 Acc: 0.0828
val Loss: 3.7354 Acc: 0.1210

Epoch 10/179
----------
train Loss: 3.6863 Acc: 0.0909
val Loss: 3.6604 Acc: 0.1073

Epoch 11/179
----------
train Loss: 3.7309 Acc: 0.0837
val Loss: 3.8037 Acc: 0.0959

Epoch 12/179
----------
train Loss: 3.6435 Acc: 0.1046
val Loss: 3.6993 Acc: 0.0913

Epoch 13/179
----------
train Loss: 3.5845 Acc: 0.1164
val Loss: 3.5788 Acc: 0.1438

Epoch 14/179
----------
train Loss: 3.5467 Acc: 0.1273
val Loss: 3.5541 Acc: 0.1050

Epoch 15/179
----------
train Loss: 3.4941 Acc: 0.1258
val Loss: 3.5708 Acc: 0.1233

Epoch 16/179
----------
train Loss: 3.4609 Acc: 0.1290
val Loss: 3.4067 Acc: 0.1575

Epoch 17/179
----------
train Loss: 3.4229 Acc: 0.1485
val Loss: 3.3515 Acc: 0.1575

Epoch 18/179
----------
train Loss: 3.3551 Acc: 0.1528
val Loss: 3.4749 Acc: 0.1370

Epoch 19/179
----------
train Loss: 3.3181 Acc: 0.1631
val Loss: 3.2675 Acc: 0.1826

Epoch 20/179
----------
train Loss: 3.2876 Acc: 0.1700
val Loss: 3.3763 Acc: 0.1781

Epoch 21/179
----------
train Loss: 3.2191 Acc: 0.1817
val Loss: 3.2097 Acc: 0.1712

Epoch 22/179
----------
train Loss: 3.1815 Acc: 0.1903
val Loss: 3.3081 Acc: 0.2283

Epoch 23/179
----------
train Loss: 3.1329 Acc: 0.2135
val Loss: 4.3193 Acc: 0.1416

Epoch 24/179
----------
train Loss: 3.0753 Acc: 0.2092
val Loss: 3.0794 Acc: 0.2283

Epoch 25/179
----------
train Loss: 3.0357 Acc: 0.2270
val Loss: 3.0348 Acc: 0.2374

Epoch 26/179
----------
train Loss: 3.0022 Acc: 0.2327
val Loss: 3.4002 Acc: 0.1849

Epoch 27/179
----------
train Loss: 2.9125 Acc: 0.2516
val Loss: 3.4580 Acc: 0.2146

Epoch 28/179
----------
train Loss: 2.8873 Acc: 0.2602
val Loss: 2.8297 Acc: 0.2534

Epoch 29/179
----------
train Loss: 2.8337 Acc: 0.2671
val Loss: 2.9087 Acc: 0.2808

Epoch 30/179
----------
train Loss: 2.7719 Acc: 0.2726
val Loss: 2.9692 Acc: 0.2671

Epoch 31/179
----------
train Loss: 2.7799 Acc: 0.2898
val Loss: 2.8207 Acc: 0.2900

Epoch 32/179
----------
train Loss: 2.7125 Acc: 0.2984
val Loss: 2.8784 Acc: 0.2900

Epoch 33/179
----------
train Loss: 2.6882 Acc: 0.3104
val Loss: 2.6716 Acc: 0.3356

Epoch 34/179
----------
train Loss: 2.6131 Acc: 0.3239
val Loss: 2.7230 Acc: 0.3379

Epoch 35/179
----------
train Loss: 2.5675 Acc: 0.3336
val Loss: 3.0120 Acc: 0.3082

Epoch 36/179
----------
train Loss: 2.4947 Acc: 0.3408
val Loss: 2.6519 Acc: 0.3767

Epoch 37/179
----------
train Loss: 2.4683 Acc: 0.3468
val Loss: 2.4656 Acc: 0.3790

Epoch 38/179
----------
train Loss: 2.4021 Acc: 0.3674
val Loss: 2.5631 Acc: 0.3995

Epoch 39/179
----------
train Loss: 2.3967 Acc: 0.3735
val Loss: 2.9473 Acc: 0.3562

Epoch 40/179
----------
train Loss: 2.3901 Acc: 0.3720
val Loss: 2.3222 Acc: 0.4521

Epoch 41/179
----------
train Loss: 2.2865 Acc: 0.4036
val Loss: 2.4896 Acc: 0.3904

Epoch 42/179
----------
train Loss: 2.2567 Acc: 0.3981
val Loss: 2.7058 Acc: 0.3950

Epoch 43/179
----------
train Loss: 2.2054 Acc: 0.4176
val Loss: 2.6590 Acc: 0.3813

Epoch 44/179
----------
train Loss: 2.2028 Acc: 0.4202
val Loss: 2.7428 Acc: 0.4155

Epoch 45/179
----------
train Loss: 2.1755 Acc: 0.4222
val Loss: 2.3723 Acc: 0.4269

Epoch 46/179
----------
train Loss: 2.1033 Acc: 0.4405
val Loss: 2.6753 Acc: 0.3927

Epoch 47/179
----------
train Loss: 2.0802 Acc: 0.4540
val Loss: 2.3429 Acc: 0.4384

Epoch 48/179
----------
train Loss: 2.0407 Acc: 0.4606
val Loss: 2.4105 Acc: 0.4498

Epoch 49/179
----------
train Loss: 1.9863 Acc: 0.4738
val Loss: 2.3150 Acc: 0.4452

Epoch 50/179
----------
train Loss: 1.9590 Acc: 0.4726
val Loss: 2.2774 Acc: 0.4361

Epoch 51/179
----------
train Loss: 1.9189 Acc: 0.4818
val Loss: 2.4198 Acc: 0.4635

Epoch 52/179
----------
train Loss: 1.9592 Acc: 0.4838
val Loss: 2.2215 Acc: 0.4726

Epoch 53/179
----------
train Loss: 1.9139 Acc: 0.4910
val Loss: 2.2403 Acc: 0.4726

Epoch 54/179
----------
train Loss: 1.8256 Acc: 0.5090
val Loss: 2.2807 Acc: 0.4566

Epoch 55/179
----------
train Loss: 1.8088 Acc: 0.5234
val Loss: 2.0498 Acc: 0.5183

Epoch 56/179
----------
train Loss: 1.7848 Acc: 0.5185
val Loss: 2.1405 Acc: 0.4886

Epoch 57/179
----------
train Loss: 1.7577 Acc: 0.5337
val Loss: 2.1644 Acc: 0.5068

Epoch 58/179
----------
train Loss: 1.7496 Acc: 0.5265
val Loss: 2.1503 Acc: 0.4932

Epoch 59/179
----------
train Loss: 1.6825 Acc: 0.5368
val Loss: 2.3982 Acc: 0.5205

Epoch 60/179
----------
train Loss: 1.6922 Acc: 0.5434
val Loss: 2.0118 Acc: 0.5114

Epoch 61/179
----------
train Loss: 1.6837 Acc: 0.5486
val Loss: 2.3394 Acc: 0.4977

Epoch 62/179
----------
train Loss: 1.6315 Acc: 0.5532
val Loss: 1.9852 Acc: 0.5434

Epoch 63/179
----------
train Loss: 1.6335 Acc: 0.5563
val Loss: 2.1935 Acc: 0.5000

Epoch 64/179
----------
train Loss: 1.5631 Acc: 0.5787
val Loss: 2.2744 Acc: 0.5183

Epoch 65/179
----------
train Loss: 1.5359 Acc: 0.5798
val Loss: 2.4422 Acc: 0.4589

Epoch 66/179
----------
train Loss: 1.4918 Acc: 0.5858
val Loss: 2.3603 Acc: 0.4726

Epoch 67/179
----------
train Loss: 1.5021 Acc: 0.5896
val Loss: 2.7359 Acc: 0.4703

Epoch 68/179
----------
train Loss: 1.4494 Acc: 0.6048
val Loss: 2.3914 Acc: 0.5000

Epoch 69/179
----------
train Loss: 1.4384 Acc: 0.6056
val Loss: 1.9998 Acc: 0.5365

Epoch 70/179
----------
train Loss: 1.4238 Acc: 0.6019
val Loss: 1.9359 Acc: 0.5525

Epoch 71/179
----------
train Loss: 1.3852 Acc: 0.6125
val Loss: 2.0811 Acc: 0.5434

Epoch 72/179
----------
train Loss: 1.3954 Acc: 0.6159
val Loss: 2.3935 Acc: 0.4932

Epoch 73/179
----------
train Loss: 1.3734 Acc: 0.6222
val Loss: 2.1450 Acc: 0.5502

Epoch 74/179
----------
train Loss: 1.3345 Acc: 0.6351
val Loss: 5.1768 Acc: 0.2968

Epoch 75/179
----------
train Loss: 1.2940 Acc: 0.6383
val Loss: 2.6394 Acc: 0.4772

Epoch 76/179
----------
train Loss: 1.3065 Acc: 0.6374
val Loss: 2.0445 Acc: 0.5616

Epoch 77/179
----------
train Loss: 1.2542 Acc: 0.6500
val Loss: 2.5400 Acc: 0.5137

Epoch 78/179
----------
train Loss: 1.2652 Acc: 0.6566
val Loss: 1.9387 Acc: 0.5616

Epoch 79/179
----------
train Loss: 1.2563 Acc: 0.6503
val Loss: 1.9823 Acc: 0.5502

Epoch 80/179
----------
train Loss: 1.1889 Acc: 0.6690
val Loss: 2.0904 Acc: 0.5502

Epoch 81/179
----------
train Loss: 1.1947 Acc: 0.6592
val Loss: 1.9766 Acc: 0.5776

Epoch 82/179
----------
train Loss: 1.2231 Acc: 0.6595
val Loss: 2.2832 Acc: 0.5228

Epoch 83/179
----------
train Loss: 1.1640 Acc: 0.6773
val Loss: 2.0756 Acc: 0.5685

Epoch 84/179
----------
train Loss: 1.1778 Acc: 0.6747
val Loss: 2.1520 Acc: 0.5457

Epoch 85/179
----------
train Loss: 1.1593 Acc: 0.6770
val Loss: 1.9180 Acc: 0.5799

Epoch 86/179
----------
train Loss: 1.1440 Acc: 0.6887
val Loss: 2.3705 Acc: 0.5091

Epoch 87/179
----------
train Loss: 1.1087 Acc: 0.6948
val Loss: 2.3512 Acc: 0.5365

Epoch 88/179
----------
train Loss: 1.1150 Acc: 0.6922
val Loss: 1.9715 Acc: 0.5936

Epoch 89/179
----------
train Loss: 1.0656 Acc: 0.6970
val Loss: 2.5272 Acc: 0.5297

Epoch 90/179
----------
train Loss: 1.0508 Acc: 0.7036
val Loss: 2.0617 Acc: 0.5594

Epoch 91/179
----------
train Loss: 0.9994 Acc: 0.7269
val Loss: 2.2542 Acc: 0.5799

Epoch 92/179
----------
train Loss: 1.0607 Acc: 0.7002
val Loss: 1.8751 Acc: 0.6027

Epoch 93/179
----------
train Loss: 1.0038 Acc: 0.7183
val Loss: 1.9798 Acc: 0.6142

Epoch 94/179
----------
train Loss: 0.9744 Acc: 0.7237
val Loss: 1.9659 Acc: 0.5936

Epoch 95/179
----------
train Loss: 1.0339 Acc: 0.7102
val Loss: 2.2642 Acc: 0.5000

Epoch 96/179
----------
train Loss: 0.9872 Acc: 0.7203
val Loss: 2.4107 Acc: 0.5068

Epoch 97/179
----------
train Loss: 0.9430 Acc: 0.7432
val Loss: 1.9087 Acc: 0.6142

Epoch 98/179
----------
train Loss: 0.9705 Acc: 0.7297
val Loss: 1.9442 Acc: 0.5776

Epoch 99/179
----------
train Loss: 0.9559 Acc: 0.7280
val Loss: 2.1469 Acc: 0.5685

Epoch 100/179
----------
train Loss: 0.9196 Acc: 0.7529
val Loss: 2.3683 Acc: 0.5365

Epoch 101/179
----------
train Loss: 0.9398 Acc: 0.7332
val Loss: 2.2677 Acc: 0.5548

Epoch 102/179
----------
train Loss: 0.9355 Acc: 0.7412
val Loss: 1.9979 Acc: 0.6027

Epoch 103/179
----------
train Loss: 0.9004 Acc: 0.7472
val Loss: 2.2208 Acc: 0.5731

Epoch 104/179
----------
train Loss: 0.8878 Acc: 0.7495
val Loss: 2.2080 Acc: 0.5753

Epoch 105/179
----------
train Loss: 0.8751 Acc: 0.7506
val Loss: 2.4096 Acc: 0.5616

Epoch 106/179
----------
train Loss: 0.8624 Acc: 0.7653
val Loss: 2.3756 Acc: 0.5365

Epoch 107/179
----------
train Loss: 0.8711 Acc: 0.7538
val Loss: 2.1002 Acc: 0.5913

Epoch 108/179
----------
train Loss: 0.8505 Acc: 0.7635
val Loss: 2.0388 Acc: 0.5890

Epoch 109/179
----------
train Loss: 0.8581 Acc: 0.7661
val Loss: 2.3170 Acc: 0.5753

Epoch 110/179
----------
train Loss: 0.8187 Acc: 0.7687
val Loss: 2.5808 Acc: 0.5274

Epoch 111/179
----------
train Loss: 0.8508 Acc: 0.7673
val Loss: 2.4716 Acc: 0.5548

Epoch 112/179
----------
train Loss: 0.8104 Acc: 0.7730
val Loss: 2.0247 Acc: 0.6050

Epoch 113/179
----------
train Loss: 0.8531 Acc: 0.7644
val Loss: 2.3796 Acc: 0.5479

Epoch 114/179
----------
train Loss: 0.8199 Acc: 0.7721
val Loss: 1.9030 Acc: 0.6119

Epoch 115/179
----------
train Loss: 0.7952 Acc: 0.7739
val Loss: 2.7342 Acc: 0.5205

Epoch 116/179
----------
train Loss: 0.7688 Acc: 0.7819
val Loss: 2.1794 Acc: 0.5936

Epoch 117/179
----------
train Loss: 0.8355 Acc: 0.7658
val Loss: 2.4954 Acc: 0.5662

Epoch 118/179
----------
train Loss: 0.7989 Acc: 0.7750
val Loss: 1.9780 Acc: 0.5845

Epoch 119/179
----------
train Loss: 0.8124 Acc: 0.7747
val Loss: 2.3183 Acc: 0.5936

Epoch 120/179
----------
train Loss: 0.7449 Acc: 0.7899
val Loss: 2.3112 Acc: 0.5616

Epoch 121/179
----------
train Loss: 0.7808 Acc: 0.7802
val Loss: 2.6163 Acc: 0.5616

Epoch 122/179
----------
train Loss: 0.7334 Acc: 0.7982
val Loss: 2.3319 Acc: 0.5731

Epoch 123/179
----------
train Loss: 0.7463 Acc: 0.7899
val Loss: 2.4265 Acc: 0.5571

Epoch 124/179
----------
train Loss: 0.7602 Acc: 0.7936
val Loss: 2.3456 Acc: 0.5731

Epoch 125/179
----------
train Loss: 0.7391 Acc: 0.7868
val Loss: 2.0327 Acc: 0.6005

Epoch 126/179
----------
train Loss: 0.7409 Acc: 0.7911
val Loss: 2.2534 Acc: 0.5822

Epoch 127/179
----------
train Loss: 0.7776 Acc: 0.7830
val Loss: 2.1315 Acc: 0.6187

Epoch 128/179
----------
train Loss: 0.7164 Acc: 0.7991
val Loss: 4.2667 Acc: 0.4132

Epoch 129/179
----------
train Loss: 0.7285 Acc: 0.7997
val Loss: 2.1452 Acc: 0.5982

Epoch 130/179
----------
train Loss: 0.7052 Acc: 0.8028
val Loss: 2.2752 Acc: 0.5594

Epoch 131/179
----------
train Loss: 0.7001 Acc: 0.7954
val Loss: 2.2780 Acc: 0.5959

Epoch 132/179
----------
train Loss: 0.6917 Acc: 0.8060
val Loss: 2.6091 Acc: 0.5388

Epoch 133/179
----------
train Loss: 0.6848 Acc: 0.8091
val Loss: 2.0010 Acc: 0.6164

Epoch 134/179
----------
train Loss: 0.7232 Acc: 0.7922
val Loss: 2.2030 Acc: 0.6027

Epoch 135/179
----------
train Loss: 0.7154 Acc: 0.7968
val Loss: 4.8706 Acc: 0.3858

Epoch 136/179
----------
train Loss: 0.6712 Acc: 0.8085
val Loss: 2.7823 Acc: 0.5228

Epoch 137/179
----------
train Loss: 0.6827 Acc: 0.8028
val Loss: 2.6088 Acc: 0.5320

Epoch 138/179
----------
train Loss: 0.6432 Acc: 0.8197
val Loss: 2.1419 Acc: 0.5753

Epoch 139/179
----------
train Loss: 0.6669 Acc: 0.8114
val Loss: 1.9737 Acc: 0.6187

Epoch 140/179
----------
train Loss: 0.6634 Acc: 0.8183
val Loss: 1.9672 Acc: 0.6005

Epoch 141/179
----------
train Loss: 0.6381 Acc: 0.8166
val Loss: 2.5829 Acc: 0.5183

Epoch 142/179
----------
train Loss: 0.6844 Acc: 0.8085
val Loss: 2.1724 Acc: 0.5731

Epoch 143/179
----------
train Loss: 0.6685 Acc: 0.8171
val Loss: 2.7850 Acc: 0.5160

Epoch 144/179
----------
train Loss: 0.6456 Acc: 0.8309
val Loss: 3.3341 Acc: 0.4635

Epoch 145/179
----------
train Loss: 0.6124 Acc: 0.8169
val Loss: 2.8461 Acc: 0.5274

Epoch 146/179
----------
train Loss: 0.6308 Acc: 0.8223
val Loss: 2.2421 Acc: 0.5845

Epoch 147/179
----------
train Loss: 0.6293 Acc: 0.8191
val Loss: 2.3863 Acc: 0.5936

Epoch 148/179
----------
train Loss: 0.6227 Acc: 0.8217
val Loss: 2.2264 Acc: 0.6073

Epoch 149/179
----------
train Loss: 0.5863 Acc: 0.8298
val Loss: 2.1868 Acc: 0.5959

Epoch 150/179
----------
train Loss: 0.6075 Acc: 0.8315
val Loss: 2.8176 Acc: 0.5388

Epoch 151/179
----------
train Loss: 0.6031 Acc: 0.8243
val Loss: 2.2890 Acc: 0.5662

Epoch 152/179
----------
train Loss: 0.6106 Acc: 0.8263
val Loss: 2.3069 Acc: 0.5753

Epoch 153/179
----------
train Loss: 0.5981 Acc: 0.8323
val Loss: 2.5973 Acc: 0.5708

Epoch 154/179
----------
train Loss: 0.6085 Acc: 0.8203
val Loss: 2.3169 Acc: 0.5753

Epoch 155/179
----------
train Loss: 0.6080 Acc: 0.8246
val Loss: 2.2940 Acc: 0.5799

Epoch 156/179
----------
train Loss: 0.5429 Acc: 0.8455
val Loss: 2.4767 Acc: 0.5890

Epoch 157/179
----------
train Loss: 0.5875 Acc: 0.8315
val Loss: 2.4424 Acc: 0.5708

Epoch 158/179
----------
train Loss: 0.5778 Acc: 0.8392
val Loss: 2.7869 Acc: 0.5548

Epoch 159/179
----------
train Loss: 0.6032 Acc: 0.8286
val Loss: 2.8085 Acc: 0.5114

Epoch 160/179
----------
train Loss: 0.5615 Acc: 0.8375
val Loss: 2.2966 Acc: 0.5845

Epoch 161/179
----------
train Loss: 0.5666 Acc: 0.8435
val Loss: 2.1377 Acc: 0.6027

Epoch 162/179
----------
train Loss: 0.5569 Acc: 0.8435
val Loss: 2.5531 Acc: 0.5616

Epoch 163/179
----------
train Loss: 0.5943 Acc: 0.8323
val Loss: 2.4649 Acc: 0.5753

Epoch 164/179
----------
train Loss: 0.5820 Acc: 0.8415
val Loss: 2.3717 Acc: 0.5662

Epoch 165/179
----------
train Loss: 0.5476 Acc: 0.8458
val Loss: 3.1483 Acc: 0.4863

Epoch 166/179
----------
train Loss: 0.5834 Acc: 0.8312
val Loss: 2.5073 Acc: 0.5822

Epoch 167/179
----------
train Loss: 0.5479 Acc: 0.8452
val Loss: 2.3366 Acc: 0.6119

Epoch 168/179
----------
train Loss: 0.5694 Acc: 0.8355
val Loss: 2.3086 Acc: 0.6050

Epoch 169/179
----------
train Loss: 0.5519 Acc: 0.8412
val Loss: 2.4538 Acc: 0.5890

Epoch 170/179
----------
train Loss: 0.5535 Acc: 0.8415
val Loss: 2.2176 Acc: 0.6301

Epoch 171/179
----------
train Loss: 0.5721 Acc: 0.8352
val Loss: 2.2416 Acc: 0.6142

Epoch 172/179
----------
train Loss: 0.5189 Acc: 0.8512
val Loss: 2.3718 Acc: 0.5936

Epoch 173/179
----------
train Loss: 0.5194 Acc: 0.8587
val Loss: 2.3277 Acc: 0.5731

Epoch 174/179
----------
train Loss: 0.5104 Acc: 0.8544
val Loss: 2.3767 Acc: 0.5571

Epoch 175/179
----------
train Loss: 0.5324 Acc: 0.8587
val Loss: 2.1927 Acc: 0.6301

Epoch 176/179
----------
train Loss: 0.5092 Acc: 0.8544
val Loss: 2.9433 Acc: 0.5160

Epoch 177/179
----------
train Loss: 0.5419 Acc: 0.8444
val Loss: 2.4956 Acc: 0.5776

Epoch 178/179
----------
train Loss: 0.5367 Acc: 0.8515
val Loss: 2.4294 Acc: 0.5639

Epoch 179/179
----------
train Loss: 0.5462 Acc: 0.8504
val Loss: 2.4499 Acc: 0.5662

Training complete in 136m 4s
Best val Acc: 0.630137
creating directory:  /workspace/ruilei/hw/result/taskC_2019-04-15-16_16
